[00:00:00] I think maybe the most interesting minus
[00:00:02] one moment I had was when I was nearing
[00:00:04] the end of my PhD. The thing that I was
[00:00:07] working on was super intellectually
[00:00:10] stimulating. I was working on this stuff
[00:00:12] called dynamic binary translation. And I
[00:00:14] was just like, yeah, I got to go find
[00:00:16] something to do where the thing that's
[00:00:19] the first priority is impact. So I left
[00:00:22] academia. I got super lucky. Got a job
[00:00:25] at Google.
[00:00:26] >> How should a founder think about like
[00:00:28] building in this world today? A lot of
[00:00:29] it is signal to noise and like there's
[00:00:31] just a bunch of false signal out there
[00:00:33] right now. Like you got a bunch of
[00:00:34] people whose business model is getting
[00:00:37] clicks on articles. If you believe the
[00:00:40] things that that particular part of the
[00:00:41] ecosystem is uh sending to you in terms
[00:00:44] of feedback, it could be that you're
[00:00:45] steering yourself in exactly the wrong
[00:00:47] direction. I think the chat GPT example
[00:00:50] is like maybe one of the most
[00:00:52] instructive things in the world for
[00:00:54] entrepreneurs right now. So the model
[00:00:56] that became the engine for chat GPT was
[00:01:00] pretty old and like not a single one of
[00:01:03] us looked at this thing and said, "Oh my
[00:01:05] god, like this is going to be the next
[00:01:06] great consumer product that's going to
[00:01:08] potentially become a trillion dollar
[00:01:10] company." There are these nuggets that
[00:01:12] are out there right now that are
[00:01:14] extraordinarily valuable that if you
[00:01:17] just did the damned experiment, the cost
[00:01:20] of doing the experiments has never been
[00:01:22] cheaper. So do the damned experiments.
[00:01:24] try things.
[00:01:34] Welcome everyone to SBC and to this uh
[00:01:37] minus one fireside chat with Kevin. Uh
[00:01:40] Kevin is someone I've known for a long
[00:01:42] time now. I think we probably met 15
[00:01:43] years ago.
[00:01:44] >> Yeah.
[00:01:45] >> Um and you know he's obviously h
[00:01:49] you talk about you. uh
[00:01:53] um but it's you know he's been someone
[00:01:56] that I've definitely looked up to in my
[00:01:58] own career kind of in his trajectory as
[00:02:00] kind of a both as an engineer as an
[00:02:02] engineering leader. One of the things
[00:02:04] that has always been striking about
[00:02:06] Kevin is his authenticity right in terms
[00:02:09] of who he is what he stands for and what
[00:02:12] he wants to build. And I think that some
[00:02:14] of the work that he's doing at Microsoft
[00:02:16] right now is just inspiring in terms of
[00:02:18] bringing what is probably the most
[00:02:20] interesting technology I think of our
[00:02:22] career so far uh out to a lot of people.
[00:02:25] Microsoft is at the forefront of this.
[00:02:27] So this should be fun. We're going to
[00:02:29] try to keep this uh fun, controversial
[00:02:32] uh and hopefully also interesting. So
[00:02:34] Kevin, you know, at SPC a lot of folks
[00:02:37] come here because they're trying to
[00:02:38] figure out uh what to work on next. Yep.
[00:02:41] >> Right. They're kind of in between, we
[00:02:43] call it the minus one phase where
[00:02:45] they're trying, you know, they might
[00:02:46] have been rolling off a a PhD or a
[00:02:49] startup or a stint in a larger company
[00:02:50] and they come here to figure out what to
[00:02:52] work on next. And a central question
[00:02:55] that we think about is how do you find
[00:02:58] great problems, right? Like how do you
[00:03:01] go through that journey of minus one? So
[00:03:03] as you look back in your own career and
[00:03:05] you've kind of obviously PhD to Google
[00:03:08] to ADM Mob back to Google to LinkedIn to
[00:03:11] Microsoft, how have you uh tackled that
[00:03:15] question and you know in between those
[00:03:16] you were definitely in some minus one
[00:03:18] zones yourself?
[00:03:19] >> Yeah.
[00:03:19] >> So how did you kind of like navigate
[00:03:21] those times in your own career?
[00:03:23] >> Yeah. Yeah, I mean I think maybe the
[00:03:24] most interesting minus one moment I had
[00:03:27] was when I was um nearing the end of my
[00:03:31] PhD and I thought I was going to be a
[00:03:33] computer science professor and it was
[00:03:35] like all all I wanted to be from the
[00:03:38] time I was 16 years old until yeah I was
[00:03:41] in my late 20s and I just had this
[00:03:45] epiphany that uh the thing that I was
[00:03:48] working on was super intellectually
[00:03:50] stimulating. It was really really
[00:03:52] interesting. um to me and not really all
[00:03:56] that interesting to anyone else. Uh like
[00:03:59] so I I was I was a compiler optimization
[00:04:02] programming language computer
[00:04:03] architecture person. I was working on
[00:04:04] this stuff called dynamic binary
[00:04:06] translation and I thought it was great.
[00:04:08] Um and in success
[00:04:13] this thing was going to make a bunch of
[00:04:15] stuff temporally like a few percentage
[00:04:18] points better on a benchmark. I was
[00:04:20] going to publish some papers, like a few
[00:04:22] dozen people were going to like read
[00:04:25] them and cite them and like I was going
[00:04:28] to have spent all of this energy like
[00:04:30] having marginal impact. Uh, and like you
[00:04:32] sort of couple that with the fact that I
[00:04:34] was broke. Uh, you know, I was making 18
[00:04:36] grand a year like trying to take care of
[00:04:38] my mom and brother, you know, pay my
[00:04:39] rent and um, you know, my car payment
[00:04:43] and I was just like, "Yeah, I got to go
[00:04:46] find something to do." uh where the
[00:04:49] thing that's the first priority is
[00:04:52] impact. Um and so I left academia. I got
[00:04:57] super lucky uh and got a job at Google
[00:05:00] before the IPO. Um you know is really
[00:05:03] funny like I didn't even know why search
[00:05:06] was an interesting problem. I just knew
[00:05:08] that a bunch of my compiler buddies, you
[00:05:10] know, ORS and Allan Eustus and Jeff Dean
[00:05:13] and Sanjay and like it just like an
[00:05:15] inordinate number of like you know
[00:05:17] system software people had gone to work
[00:05:18] for Google. I didn't know why but I was
[00:05:20] like all right well resume in when I
[00:05:22] joined the thing that I saw at Google
[00:05:26] was the same thing that I saw for myself
[00:05:28] as an academic that people were
[00:05:31] gravitating towards these intellectually
[00:05:34] interesting problems that if you just
[00:05:36] looked at them even a little bit you
[00:05:39] were going to notice that they weren't
[00:05:40] going to have any impact. And so like I
[00:05:42] was just determined when I got to
[00:05:43] Google. It's like I'm gonna go find a
[00:05:45] thing uh and I don't care how
[00:05:47] intellectually
[00:05:49] interesting it is like this thing is
[00:05:51] going to have impact. And you know it
[00:05:53] was this weird thing. It was uh the
[00:05:55] called the ads approval bin automation
[00:05:58] system. Uh and the problem that it was
[00:06:00] solving was at the time every ad that
[00:06:03] was going to run on Google had to be
[00:06:06] reviewed by human beings. Um, and so we
[00:06:09] couldn't hire people fast enough to do
[00:06:12] all of the reviewing. And the review
[00:06:14] stuff was very simple. Uh, most of the
[00:06:17] time it was like you can't use
[00:06:18] superlative in your ad copy. Like you
[00:06:20] can't have repeated punctuation. Like
[00:06:22] you can't have three exclamation marks
[00:06:24] at the end of something. But like some
[00:06:25] of it was complicated like you can't
[00:06:27] have advertising that is for adult
[00:06:29] content if the keywords aren't uh like
[00:06:32] adult content. Uh so like you can't have
[00:06:34] deceptive uh like redirects on clicks.
[00:06:36] it was blocking $50 million worth of
[00:06:39] inventory a day from running, you know,
[00:06:41] so it was like a big dollar value
[00:06:44] problem. Um, and like some of it wasn't
[00:06:46] like super sexy to fix like the the you
[00:06:49] know this no superlatives and repeated
[00:06:51] punctuation was like really like a
[00:06:52] regular expression like you so you just
[00:06:54] needed to build some plumbing that was
[00:06:56] going to solve a workflow problem and
[00:06:58] then like you needed to build a little
[00:06:59] bit of machine learning and so like you
[00:07:01] know it was
[00:07:02] >> we won a founders award for it like I
[00:07:04] you know I forget what the time period
[00:07:06] was but like there was some point like
[00:07:08] pretty quickly where it had saved about
[00:07:10] a billion dollars in operating costs at
[00:07:11] the company and and it was not by any
[00:07:15] measure whatsoever technically the most
[00:07:17] complicated or interesting problem that
[00:07:19] was available to be solved at the time
[00:07:21] but it was like a super
[00:07:23] >> impactful problem a thing that I've
[00:07:25] tried to carry forward since then it's
[00:07:28] like everything I do like I want to look
[00:07:30] at it first through the lens of impact
[00:07:32] and then through the lens of technical
[00:07:35] interest
[00:07:35] >> one of the things that's hard when you
[00:07:37] kind of leave a larger company and kind
[00:07:40] of awesome companies right like a Google
[00:07:41] or Microsoft or Facebook is that you're
[00:07:44] Right? In each of those companies,
[00:07:45] there's a ton of opportunity like
[00:07:47] despite their size to go work on
[00:07:49] something that is going to probably
[00:07:51] impact tens of millions of users and
[00:07:53] kind of like make tens of millions of
[00:07:54] dollars. And then when you come out of
[00:07:56] that, you know, at a startup, you're
[00:07:58] like affecting most of the like in the
[00:07:59] beginning, you know, probably one year
[00:08:00] zero users.
[00:08:01] >> Yep.
[00:08:02] >> And you're making negative money, right?
[00:08:04] Um, so how do you apply some of that
[00:08:08] framework if you are truly kind of like
[00:08:11] in some ways pre-revenue pre-users and
[00:08:13] you've worked with a bunch of startups
[00:08:14] over the years as well. So that I I've
[00:08:17] noticed that people who have seen and I
[00:08:18] suffered from this when I left Facebook.
[00:08:20] I think that part of what I really
[00:08:22] suffered from was that my only lens of
[00:08:24] impact was frankly like tens of hundreds
[00:08:27] of millions of users and kind of like
[00:08:29] this well we like yeah I work in ads as
[00:08:31] well so like definitely making money. So
[00:08:33] anything I did at my startup just felt
[00:08:35] as though I wasn't having impact and I
[00:08:37] really struggled with this at uh at Cove
[00:08:39] which was my own startup.
[00:08:40] >> Yeah.
[00:08:40] >> Yeah.
[00:08:41] >> Like a bunch of the things I can say
[00:08:43] here are just pathy
[00:08:46] not not novel insight. I mean I I think
[00:08:49] you actually do have to um you when
[00:08:53] you're pre-revenue and you're hunting
[00:08:55] for product market fit, you just have to
[00:08:56] be really really relentless about your
[00:08:59] willingness to pivot. uh because you
[00:09:02] know effectively what you're doing is
[00:09:04] exploring this optimization landscape uh
[00:09:06] like you have no idea like exactly where
[00:09:08] you're going and like you just have to
[00:09:10] be quick about running experiments.
[00:09:12] Yeah. Like a super super dangerous trap
[00:09:14] that people get into all the time when
[00:09:17] they're uh pre-product fit is getting
[00:09:21] super in love with an idea or super in
[00:09:23] love with a piece of technology. And you
[00:09:25] know like you just sort of spend way way
[00:09:27] way too much time uh working on that
[00:09:31] idea
[00:09:32] uh in a vacuum of real feedback about
[00:09:35] whether or not it's useful. Um and so
[00:09:38] just getting to the point where you can
[00:09:39] try things as quickly as possible. But
[00:09:41] that's not unique, right? Like everybody
[00:09:42] should know that at this point, you
[00:09:43] know, the thing that I've always tried
[00:09:45] to do, you know, maybe to my own
[00:09:46] detriment is like when I went when I
[00:09:48] left Google and and um did a startup,
[00:09:52] >> I
[00:09:54] could have done a whole bunch of things
[00:09:55] and like the the thing that I chose to
[00:09:58] do uh was a thing that just very clearly
[00:10:01] made sense to me was going to have
[00:10:03] impact. So, it was mobile advertising
[00:10:05] and this was like right before uh the
[00:10:08] iPhone. I knew the iPhone was coming.
[00:10:10] Uh, and I had a suspicion that it was
[00:10:13] going to change the entire landscape.
[00:10:15] And you kind of knew that like if you
[00:10:17] just sort of looked at the technology
[00:10:19] trends, you knew that not necessarily
[00:10:21] the iPhone was going to happen, but uh,
[00:10:23] you know, your
[00:10:24] >> wireless networks were getting faster,
[00:10:26] like batteries were getting faster, uh,
[00:10:28] like uh, screen technology was getting
[00:10:30] better and better. Like you knew that at
[00:10:32] some point like a whole bunch of the
[00:10:34] power of computers was going to converge
[00:10:36] on like this mobile form factor. Um, and
[00:10:38] if you believe that, uh, like you know
[00:10:41] that people are going to be be building
[00:10:42] mobile apps and like as soon as you've
[00:10:44] got mobile apps, you need distribution
[00:10:45] for them and you need monetization for
[00:10:47] them and so like I'm going to go build
[00:10:49] an ad network. It was real clear to me
[00:10:53] uh that I I should go join this company
[00:10:57] because I knew a bunch about advertising
[00:10:59] from working at Google and I knew
[00:11:02] nothing at all about a whole bunch of
[00:11:04] other things uh that where that startup
[00:11:06] and so like I had a little bit of
[00:11:07] comfort zone on like here's a problem
[00:11:09] space I understand so like I know I can
[00:11:11] make progress on that and that will give
[00:11:12] me an opportunity to go learn a whole
[00:11:14] bunch of other things that like I I
[00:11:15] don't know whether I'm good at or not.
[00:11:17] >> So ADM was like 2007 and 8. Yeah. 2007
[00:11:21] and we got bought by Google in 2010,
[00:11:24] >> which is before kind of like iPhones
[00:11:27] were, you know, big thing, right? So you
[00:11:29] guys, in some ways you were early, so
[00:11:31] you had a bet.
[00:11:32] >> Yeah.
[00:11:32] >> And and look, it was the it was the
[00:11:34] fastest growing thing I think still I've
[00:11:36] ever seen. Like we were doubling
[00:11:38] everything about the business was
[00:11:39] doubling once every four and a half
[00:11:40] months. Uh I mean it was just super
[00:11:44] challenging just to keep the
[00:11:45] infrastructure up and running.
[00:11:46] >> I think there's great advice in there. I
[00:11:48] think one of the things that I find,
[00:11:50] you know, because the natural temptation
[00:11:53] is you come out of Facebook, you come
[00:11:54] out of like these other places, you're
[00:11:56] like, "Oh, let me go make sure I talk to
[00:11:58] enough customers." Like your proxy for
[00:12:00] impact becomes near-term customer
[00:12:02] validation. I think that's very
[00:12:03] important. But I actually think the
[00:12:05] first thing you have to answer is having
[00:12:07] a very strongly opinionated point of
[00:12:09] view on where the world is going to be 5
[00:12:11] years from now. And then your near-term
[00:12:14] product is your best in some ways
[00:12:16] product against that long-term future.
[00:12:19] >> But the near-term product cannot be an
[00:12:22] artifact in and of itself because that
[00:12:24] is often probably like if it's a sorry
[00:12:26] if it's an artifact in and of itself,
[00:12:28] the big companies are just going to do
[00:12:29] it because they're actually really
[00:12:30] well-run big companies.
[00:12:32] >> Yeah. And look, I think you also I mean
[00:12:34] there's a bunch of things that come with
[00:12:35] that. So you have to be very honest with
[00:12:37] yourself about the difference between
[00:12:39] what you wish is going to happen in the
[00:12:40] future and what must happen in the
[00:12:42] future. And they're very very different
[00:12:44] things. And like a lot of us uh like you
[00:12:46] know I I think it's especially hard for
[00:12:48] entrepreneurs because entrepreneurs are
[00:12:50] like used to being able to force their
[00:12:52] will on the world. Uh and so like you
[00:12:55] can convince yourself that you can make
[00:12:56] a whole bunch of things happen. And like
[00:12:58] thank goodness you you actually can. But
[00:13:00] that's different from what must happen.
[00:13:02] like the thing that's going to happen
[00:13:04] whether or not you uh like apply your
[00:13:06] will to it. And it's really really good
[00:13:09] I think to have a point of view about uh
[00:13:11] like what must happen because like
[00:13:12] that's going to be the landscape in
[00:13:15] which everything you do is going to have
[00:13:17] to operate. Correct.
[00:13:18] >> Yeah. It's interesting kind of the the
[00:13:20] two traits that you often are very
[00:13:22] highly uh correlated in entrepreneurs is
[00:13:25] obviously super high agency. So your
[00:13:27] desire to go bend the world to your
[00:13:28] will, but then also like delusional
[00:13:30] optimism, which you need to be an
[00:13:32] entrepreneur, right? Which is that you
[00:13:33] can choose to wake up and see all the
[00:13:35] [ __ ] that can go wrong with your company
[00:13:36] every day, or you can choose to wake up
[00:13:38] and be like, "No, I'm still going to
[00:13:39] build something, you know, [ __ ]
[00:13:40] amazing, you know, that will exist."
[00:13:42] >> And but then sometimes those two
[00:13:44] conflate in weird ways that I think are
[00:13:47] pretty hard to to know in the moment. Uh
[00:13:50] yeah.
[00:13:51] >> Yeah. Well, and you know, you you have
[00:13:53] to compensate for that by, you know, to
[00:13:55] my earlier point, like you just got to
[00:13:57] seek a lot of feedback. Uh like you have
[00:14:00] to
[00:14:01] >> make sure that your hypotheses are
[00:14:03] falsifiable. Uh I mean, like there's
[00:14:05] just a bunch of stuff. Uh you know, and
[00:14:08] again, I think all of you get it, right?
[00:14:09] Uh like these are these are not uh
[00:14:12] >> you know, groundbreaking things, but
[00:14:13] like sometimes like psychologically
[00:14:15] they're difficult to like have
[00:14:17] discipline around. Kevin, if you uh you
[00:14:20] know, one of the things that we often
[00:14:23] kind of building off of that, right, one
[00:14:24] of the the hardest questions I think as
[00:14:26] an entrepreneur is, is this a bad idea
[00:14:29] or is this just too early?
[00:14:31] >> Should I kind of like stick around the
[00:14:33] hoop for another 12 18 months? Can I
[00:14:36] like, you know, take some kind of
[00:14:38] glimmers of hope from like some of the
[00:14:39] early things we're seeing? So, I'm going
[00:14:40] to put this question into context,
[00:14:42] though. Um, one of the things that I
[00:14:45] think is not talked about enough is how
[00:14:50] much how early Microsoft started
[00:14:53] supporting OpenAI, right? like this is
[00:14:56] not a recent like you know 2023 24 story
[00:15:00] like what did you see in the early days
[00:15:02] cuz they were you know it's just a
[00:15:03] research lab and you know prior to GPT3
[00:15:06] GPT 1 and 2 were kind of cool but you
[00:15:09] know you was there a glimmer of
[00:15:12] capability or potential that you perhaps
[00:15:16] saw before most people
[00:15:17] >> well I I think it gets back to this uh
[00:15:20] like trying to understand
[00:15:22] uh like what must happen in the world.
[00:15:25] And so there were a handful of
[00:15:28] technological things that had happened
[00:15:30] in machine learning
[00:15:33] um leading up to the point where we did
[00:15:37] that first investment in or the first
[00:15:39] big investment in OpenAI. that
[00:15:43] were the first glimmers that I had ever
[00:15:45] seen in my career of uh these AI systems
[00:15:49] becoming
[00:15:50] generalizable platform components like
[00:15:53] things where you're going to invest a
[00:15:55] ton of time and energy in like training
[00:15:58] a thing and it was going to be useful
[00:16:00] for like more than one narrow thing. So,
[00:16:02] you know, like for a while I um yeah,
[00:16:04] ran the team at Google that did the uh
[00:16:07] CTR uh uh the click-through rate
[00:16:09] prediction models uh for advertising,
[00:16:11] which is like at the time was like the
[00:16:13] singular thing that made the ad system
[00:16:16] uh work well.
[00:16:18] >> So, one thing you looked out for Google
[00:16:19] earnings reports like what are the CTR
[00:16:21] rate, you know? Yeah.
[00:16:22] >> You basically it was the quality signal
[00:16:24] for the for the ad auction. Um and if
[00:16:27] you didn't have it, like it just would
[00:16:28] have like like the ad system just
[00:16:30] wouldn't have worked. Um, and it was an
[00:16:33] extraordinary system. It still may even
[00:16:36] today be like the most valuable machine
[00:16:38] learning system. Uh, like you know,
[00:16:40] Meta's probably got one that's uh like
[00:16:42] relatively equivalent. Um, but like
[00:16:44] that, you know, the thing just prints
[00:16:46] money, relatively unsophisticated
[00:16:48] algorithm, like hugely complicated uh
[00:16:51] infrastructure, only useful for that one
[00:16:54] thing. like no matter how much money you
[00:16:57] invest, uh it's only ever going to be
[00:17:00] good at uh like predicting the
[00:17:02] click-through rate of not ads in
[00:17:05] general, but the ads in this particular
[00:17:07] context. Um but you know, like and and
[00:17:10] everybody had been looking for, you
[00:17:11] know, is transfer learning ever going to
[00:17:12] work? Are we ever going to be able to
[00:17:14] have generalizable things? Uh and it had
[00:17:17] begun to work. uh like there were you
[00:17:20] know a handful of papers uh that were
[00:17:24] demonstrating the uh you know the
[00:17:27] ability of these models to generalize
[00:17:29] and open AAI just happened to have like
[00:17:32] a very very credible theory about how uh
[00:17:37] how as you apply more compute and data
[00:17:40] scale uh to training these systems like
[00:17:43] these are the ways in which they become
[00:17:44] more general and more capable. they had
[00:17:47] run a couple of rounds of uh these
[00:17:49] experiments and like they were on
[00:17:52] prediction for the experiments they ran
[00:17:54] and like it's it's a little more
[00:17:56] complicated than this but it you know
[00:17:57] effectively the conversation was we need
[00:17:59] a billion dollars to turn over the next
[00:18:00] card. Um, and for us, uh, like the
[00:18:05] possibility that this was going to be
[00:18:07] the most efficient way to like make
[00:18:10] progress towards getting to
[00:18:12] generalizable models where you could
[00:18:14] sort of have a world where you really
[00:18:17] didn't need a 100 teams at Microsoft
[00:18:20] each independently building their narrow
[00:18:22] vertical model for a particular thing,
[00:18:24] but you could collapse a whole bunch of
[00:18:25] those down into like a single piece of
[00:18:28] platform infrastructure that you could
[00:18:30] invest in that had composability that
[00:18:32] acted like you know the rest of the
[00:18:34] software engineering universe that like
[00:18:36] we you know known to love. Um like that
[00:18:40] seemed like not a large amount of money
[00:18:43] to to me at least uh to go run that
[00:18:46] experiment.
[00:18:47] >> Yep. Well, Microsoft does have a big
[00:18:50] balance sheet but still a billion
[00:18:52] dollars to kind of like give over to
[00:18:53] what was essentially not even a real
[00:18:56] startup. It was kind of like a a labish.
[00:19:00] I mean, Ilia came to SPC, you know, back
[00:19:02] in 20 late 2016, early 2017, and his
[00:19:05] whole thesis was simply like, yo, I just
[00:19:08] believe compute, like, you know, scale
[00:19:09] will solve everything. I just need more
[00:19:11] GPUs. I just need to throw more machine.
[00:19:12] Like, and we, you know, we like, yeah,
[00:19:14] great. Go talk to the people at SPC and
[00:19:16] maybe you can. And, you know, a few of
[00:19:17] the early OpenAI folks went there via
[00:19:20] SPC, but it was all kind of like very
[00:19:22] like, you know, mad cap kind of, you
[00:19:24] know, mad scientist vibe.
[00:19:25] >> Yeah. Um so to give a billion dollars at
[00:19:28] that point I think is a story that is
[00:19:30] under reportported on the early days
[00:19:31] like 2017 2018 um and certainly more
[00:19:35] than I think Google was giving to the
[00:19:37] deep learning folks and Google brain at
[00:19:39] that point.
[00:19:39] >> Yeah I mean it was it was a substantial
[00:19:41] amount of uh like funding um you know
[00:19:44] mostly for compute um you know so most
[00:19:46] most of what it bought was like a really
[00:19:48] large supercomputer that we were going
[00:19:50] to go uh go build. It just seemed super
[00:19:53] clear to me. It It's like so if if you
[00:19:56] look at where the technology was
[00:19:59] trending, you're it was one of those
[00:20:01] things where like now that we know that
[00:20:03] like there are these incipient scaling
[00:20:06] laws and like generalization is going to
[00:20:09] work, then you you know that like you're
[00:20:12] just going to have a huge amount of
[00:20:13] pressure on the scaling itself because
[00:20:15] this thing is going to be super useful.
[00:20:17] Like you you you could ask the following
[00:20:20] question like if this thing existed and
[00:20:22] you are a hypers scale cloud company
[00:20:24] could you uh conceive of having a good
[00:20:27] business without it like the answer to
[00:20:29] that is no. The only remaining question
[00:20:32] you have then is like what is the most
[00:20:34] efficient uh like riskadjusted path for
[00:20:36] like getting to this world that you know
[00:20:39] is going to emerge.
[00:20:41] Yeah, I think that's actually a really
[00:20:42] interesting framing for founders in the
[00:20:45] early stages. And if you don't mind, I
[00:20:48] mean, one of the things that I find
[00:20:49] particularly frustrating is when people
[00:20:50] come and talk to me about like, you
[00:20:52] know, TAM and market sizes because I'm
[00:20:54] just like most of the time they're
[00:20:56] assuming that the market stays constant,
[00:20:59] right? And like you're kind of it's it
[00:21:00] feels very zero sum whenever anybody
[00:21:02] talks about TAM. A much more interesting
[00:21:05] thing is if I did this then how how does
[00:21:08] a market react to that in ways that
[00:21:10] hopefully are much bigger. So if I built
[00:21:13] this thing then how big could this
[00:21:15] become is a much more interesting
[00:21:17] question and a much more creative
[00:21:19] question I think as opposed to simply
[00:21:21] like there exists a thing that I must go
[00:21:23] get some part of. I think also if the
[00:21:26] thing that you want to take a swing at
[00:21:28] is the largest possible thing like all
[00:21:30] of the largest possible things tend to
[00:21:32] have been some kind of positive
[00:21:34] something.
[00:21:34] >> Yes.
[00:21:35] >> Like Google was positive sum, right? Uh
[00:21:39] Facebook was positive sum. Like all of
[00:21:41] these things uh like they it's not like
[00:21:44] they were
[00:21:46] >> taking away uh like someone else's share
[00:21:49] of a fixedized TAM. is like they created
[00:21:51] like gigantic new industries.
[00:21:54] >> Could not agree more. I think that the
[00:21:56] technology is best when it's essentially
[00:21:58] positive sum and it just creates like
[00:22:00] more kind of you know more abundance and
[00:22:02] kind of that can flow through to all the
[00:22:04] different constituents. Um moving on
[00:22:07] perhaps more specifically to the AI
[00:22:09] landscape. You've been around for a
[00:22:12] while now. What do you think is
[00:22:14] different about starting a company in
[00:22:17] kind of this unique moment in time that
[00:22:19] we are today where it's so loud out
[00:22:22] there? There's so much money flying
[00:22:24] around. There's
[00:22:26] >> so much kind of being written about
[00:22:27] companies that some of them do or don't
[00:22:29] have. It's just very loud, right? And
[00:22:32] there's also a lot of stuff changing up
[00:22:34] and down the stack in terms of
[00:22:35] capabilities from obviously the infra
[00:22:38] you know the foundation model providers
[00:22:39] to the infrastructure providers to the
[00:22:42] uh capabilities on top. So h what is
[00:22:46] different like what is like how should a
[00:22:47] founder think about like building in
[00:22:49] this world today
[00:22:50] >> like I I agree it's uh on some
[00:22:53] dimensions uh you know super tough right
[00:22:56] now and a lot of it is signal to noise.
[00:22:59] Uh so like the thing that you need as a
[00:23:01] entrepreneur or product maker of any
[00:23:03] sort is like feedback. Like you need to
[00:23:06] you need to be hearing like whether the
[00:23:08] thing that you're doing is useful or
[00:23:10] not. And like there's just a bunch of
[00:23:11] false signal out there right now. Like
[00:23:13] you got a bunch of people whose business
[00:23:15] model is getting clicks on articles uh
[00:23:18] online or getting people to subscribe to
[00:23:21] their Substack. You've got a lot of
[00:23:23] people who they're giving you these uh
[00:23:25] investment signals. They're like, "Oh, I
[00:23:27] want to write a check into your
[00:23:28] company." Like none of those have
[00:23:29] anything at all to do with whether or
[00:23:30] not you made a useful thing. Like
[00:23:32] literally zero. Um and in many many
[00:23:34] cases it's like negative. It it is like
[00:23:37] you are letting if you believe the
[00:23:39] things that you know that particular
[00:23:41] part of the ecosystem is uh sending to
[00:23:44] you in terms of feedback. It could be
[00:23:45] that you're steering yourself in exactly
[00:23:47] the wrong direction. There's also like
[00:23:49] ego like I remember a handful of years
[00:23:51] ago like every uh every AI startup that
[00:23:53] I was uh hearing about and you know like
[00:23:57] some of the investors that I was uh you
[00:23:59] know advising on background were um like
[00:24:03] folks who were going to build their own
[00:24:04] foundation model and I'm like yeah this
[00:24:06] is craziest thing I've ever heard in my
[00:24:07] life like you you there is no capacity
[00:24:11] in the universe for a hundred of these
[00:24:14] startups which is where it's trending
[00:24:16] and it's certainly you know what a bunch
[00:24:17] of people want to do to like go each
[00:24:19] spend, you know, hundreds of billions of
[00:24:22] dollars like building a like frontier
[00:24:24] class foundation model that is doing
[00:24:27] exactly the same thing, you know, except
[00:24:29] for technical approach that all of these
[00:24:31] other folks are doing. Um, you know, and
[00:24:34] like I would hear from some folks, well,
[00:24:36] you know, like if I don't let them do
[00:24:38] this, then they're not going to let us
[00:24:40] invest. And I'm like, yeah, this is like
[00:24:41] just the most circular like weird thing
[00:24:43] I've ever seen. Um so you know I I think
[00:24:47] you have to be very very careful about
[00:24:50] understanding the quality of the signal
[00:24:51] that you are relying upon to make
[00:24:54] decisions. Uh but I think a lot of the
[00:24:56] stuff is the same like I think it really
[00:24:57] does get back to core product making.
[00:25:00] It's like do you have a good product
[00:25:01] thesis? Do you have a customer? like are
[00:25:04] you uh like really being faithful to the
[00:25:07] needs of those customers and like you
[00:25:09] know very quickly like building
[00:25:11] delightful things like how quickly can
[00:25:13] you get to you know your first fans who
[00:25:16] are just in love with the thing that you
[00:25:19] built for them like that's good signal
[00:25:21] um
[00:25:22] >> right
[00:25:23] >> um but yeah I mean it's a lot of com and
[00:25:25] it didn't even exist for us early in our
[00:25:27] career like it it just wasn't there like
[00:25:30] you didn't have people throwing money at
[00:25:31] you and you certainly didn't have a like
[00:25:33] this tech reporting ecosystem uh out
[00:25:36] there that's
[00:25:37] >> just you know they they need to write
[00:25:39] things so that people are going to click
[00:25:40] and subscribe otherwise they don't have
[00:25:42] a business model
[00:25:43] >> but like that's their problem that's not
[00:25:45] yours
[00:25:45] >> like at all.
[00:25:47] >> Yeah. It's interesting. I think that it
[00:25:49] was just so much quieter in the early
[00:25:50] days of Facebook. I think I look back
[00:25:52] and my life was pretty simple. You know
[00:25:54] I would basically wake up go to work
[00:25:56] code for 15 hours a day come back home
[00:25:58] sleep and just do that you know six
[00:25:59] seven days a week right and there wasn't
[00:26:01] that much in between. Um,
[00:26:04] you know, we we talked a little bit
[00:26:05] about this earlier, Kevin, which is that
[00:26:08] there's so much time, energy, money
[00:26:11] being poured into foundation models and
[00:26:14] kind of like, you know, obviously the
[00:26:15] big ones, the established ones and
[00:26:16] people are trying to do some of their,
[00:26:18] you know, the new novel takes u into,
[00:26:21] you know, post-training techniques
[00:26:23] around kind of like the amount of money
[00:26:24] that has been thrown towards RL recently
[00:26:26] has been ungodly, but the not enough in
[00:26:30] terms of actually taking these models
[00:26:33] and learning how to utilize them in
[00:26:36] settings where they can add a lot of
[00:26:38] value but still but will require kind of
[00:26:40] in some ways novel approaches and how to
[00:26:43] how to kind of like you know kind of
[00:26:45] hurt them if you will and do you think
[00:26:48] would you agree with that statement and
[00:26:49] if so what do you think is causing that
[00:26:51] >> I don't think that we've had our last
[00:26:54] discovery on uh you know like how you do
[00:26:56] pre-training or how you do do
[00:26:58] post-training like I think there's lots
[00:27:00] of technical innovation
[00:27:02] But if you want to start a company to do
[00:27:04] those things, like I think you have to
[00:27:06] you really really be careful and honest
[00:27:08] about what the business model is for
[00:27:10] that because for the most part like you
[00:27:12] build an infrastructure and like you
[00:27:13] basically are saying I'm going to create
[00:27:15] a platform company and like platform
[00:27:17] companies are about you know like
[00:27:19] absolute scale and the economies that
[00:27:21] come with the you know with the large
[00:27:23] absolute scale. Um, and like in in the
[00:27:26] limit, the only thing that matters if
[00:27:27] you're building infrastructure for other
[00:27:28] people is like, you know, is this stuff
[00:27:30] getting more capable over time? Is it
[00:27:32] getting faster? Is it getting cheaper in
[00:27:34] a world where you've got intense
[00:27:35] competition? Uh, like it's it's a, you
[00:27:38] know, like a really brutal game to go
[00:27:40] play, which is not me saying like don't
[00:27:42] go play it. Uh, like I I think it's also
[00:27:44] like a super fun game to go play, but it
[00:27:47] it's probably hard for reasons that are
[00:27:52] different than your clever take on like
[00:27:55] how to implement like a particular, you
[00:27:58] know, algorithm or a piece of the
[00:28:00] technology stack. Um, it it's just got a
[00:28:03] bunch of complexity that is in many ways
[00:28:07] divorced from the technology itself. And
[00:28:09] so like if you want to go do that like
[00:28:10] you got to get convinced that like
[00:28:12] that's the game that you want to be
[00:28:14] playing and like where you can add some
[00:28:16] valuable contribution. I think
[00:28:18] everywhere else like you've just got uh
[00:28:20] you people should be infinitely
[00:28:23] pragmatic about how it is they're going
[00:28:25] to solve like an interesting uh like
[00:28:28] customer problem. And I think we were
[00:28:30] talking about this a little bit earlier.
[00:28:32] Um, there's this gigantic capability
[00:28:36] overhang that I think we have right now
[00:28:37] with these AI systems where they already
[00:28:40] like forget about what's going to happen
[00:28:41] in a year. They're already more powerful
[00:28:43] than what people are using them for. And
[00:28:46] a lot of the, you know, a lot of the
[00:28:48] reasons that people are waiting around
[00:28:51] um and not solving problems is that, you
[00:28:55] know, some of the things that you need
[00:28:56] to do to like squeeze the capability out
[00:28:59] of these systems is just ugly looking
[00:29:02] plumbing stuff.
[00:29:03] >> Yeah.
[00:29:03] >> Or you know, like grunty product
[00:29:05] building. But like tell you what, like
[00:29:07] I'm I'm sure this is true for you as
[00:29:09] well.
[00:29:10] >> You're in a startup. Like that's kind of
[00:29:12] your life. Like it's it's more about the
[00:29:14] grind. 99% of it is just a grind.
[00:29:17] >> Yeah.
[00:29:17] >> Yeah. Yeah. Yeah.
[00:29:18] >> It's not not clever. It's grind.
[00:29:21] >> You can't be dumb, right? But uh but
[00:29:23] like it it really is uh you know, just
[00:29:27] any way possible like I'm going to drive
[00:29:30] a bulldozer through solving this
[00:29:31] problem.
[00:29:32] >> It's interesting, you know, how one of
[00:29:34] the particular areas you and I mentioned
[00:29:36] this earlier like you thinking about is
[00:29:39] how to reconcile uh the fact that these
[00:29:42] models are so general purpose, right?
[00:29:44] And we've obviously got an everexpanding
[00:29:46] context window so you can kind of tailor
[00:29:48] them to kind of what you want them to do
[00:29:50] for your particular use case. Um but
[00:29:53] ultimately there's still a huge
[00:29:56] opportunity in terms of long-term memory
[00:29:59] retrieval. You can call it maybe
[00:30:01] fine-tuning. But how again a lot of it
[00:30:03] is like how do you successfully apply
[00:30:05] these models to like you know big kind
[00:30:08] of like separate context that could be
[00:30:09] your organization, it could be your
[00:30:11] function.
[00:30:11] >> Yeah. Um, where do you think the
[00:30:13] opportunity there is?
[00:30:14] >> Yeah, I think there's a huge amount of
[00:30:15] opportunity there. Um, like I've been
[00:30:18] talking about this for a couple of years
[00:30:19] now. Um, yeah, if you just sort of scope
[00:30:23] down to agents, uh, like and you you
[00:30:25] think that the purpose of agents is to
[00:30:28] be able to uh for humans to be able to
[00:30:31] delegate increasingly complicated tasks
[00:30:34] for the agents to go complete
[00:30:35] autonomously. Uh, you know, as much of
[00:30:38] that complicated task as humanly
[00:30:39] possible. It's inconceivable that uh you
[00:30:43] could do a similar sort of delegation to
[00:30:45] a human that didn't have a functioning
[00:30:47] uh memory.
[00:30:48] >> Correct.
[00:30:48] >> Um like you you have to be able to not
[00:30:52] necessarily have the entirety of that
[00:30:56] problem scope uh like in your active
[00:30:59] memory at any one time. But like you
[00:31:00] have to be able to do retrieval. like a
[00:31:02] biological brain uh is like really good
[00:31:04] even at uh like imprecise retrieval with
[00:31:08] mechanisms to like uh drive precision uh
[00:31:11] like after you've gotten in the ballpark
[00:31:13] of
[00:31:14] >> um you know what what you need to go
[00:31:16] retrieve. But like you know agents are
[00:31:18] going to need to be able to like scratch
[00:31:21] pad their work. They're going to be able
[00:31:22] to need to remember previous interaction
[00:31:24] that they've had with you. they're going
[00:31:26] to need to be able to way more
[00:31:27] effectively than things like rag like
[00:31:29] pull things that they need to solve a
[00:31:31] problem into context. Uh and so like I
[00:31:34] think there's a bunch of there's a bunch
[00:31:36] of infrastructure that you need to build
[00:31:38] there. And like I think there's even a
[00:31:39] bunch of like application specific
[00:31:41] things that you're going to have to
[00:31:42] build there. Like if you sort of think
[00:31:43] about, you know, again, biological
[00:31:46] memory, like a bunch of us like go to
[00:31:48] university and like we get a bunch of
[00:31:49] techniques loaded into our brains, uh,
[00:31:52] that help us manage a, uh, discipline
[00:31:56] specific memory that we have. Uh, and so
[00:31:58] like I think you're going to need that
[00:32:00] in agents. It's going to be like, you
[00:32:01] know, task or product specific. And like
[00:32:03] it's not going to just drop out of the
[00:32:06] training a little bit bigger model. like
[00:32:08] someone's going to have to go do some
[00:32:10] real work to um you know plum all of
[00:32:13] that stuff all the way through.
[00:32:15] >> Yeah, I thought that um I don't know how
[00:32:17] many of you listened to uh
[00:32:20] Saskar on Dwarf case I think a week and
[00:32:23] a half ago but I thought his
[00:32:24] articulation of this was super
[00:32:26] interesting. He said that right now the
[00:32:28] focus seems to be that hey can we get
[00:32:30] these foundation models to out of the
[00:32:32] box be the world's best knowledge worker
[00:32:34] for everything whereas in reality what
[00:32:37] you really want is a really smart kind
[00:32:40] of like knowledge worker
[00:32:41] >> yes
[00:32:42] >> who can go learn about the [ __ ] you want
[00:32:44] them to do in your company right
[00:32:46] >> because it's it doesn't make sense that
[00:32:48] out of the box somebody would be like
[00:32:50] right like you know 29's good at
[00:32:52] everything because and that's kind of
[00:32:54] the approach we're taking right now okay
[00:32:56] our whole pre-training, post training,
[00:32:57] but really the point of going to
[00:32:59] university is that you have a bunch of
[00:33:00] really solid like foundations and kind
[00:33:03] of like capabilities that you can then
[00:33:04] apply in a variety of different
[00:33:06] contexts. So know it's a second thing
[00:33:08] that we need but we just built half
[00:33:09] today I think.
[00:33:10] >> Yeah, I completely agree with that. Um,
[00:33:15] amazing. Okay. Um, I guess
[00:33:18] maybe to ask the question explicitly,
[00:33:20] I've already answered this, but I'm
[00:33:21] still curious. um how exams like you
[00:33:25] know with foundation models we obviously
[00:33:28] have a number of the large labs and some
[00:33:30] of the bigger companies converging on
[00:33:31] what seem to be frontier models that
[00:33:33] have the sufficient level of I would say
[00:33:36] the feedback loops the flywheels kind of
[00:33:39] like the ability to fund them and they
[00:33:41] all I mean weirdly enough they're also
[00:33:44] giving access to it to everyone in the
[00:33:46] world which is kind of incredible if you
[00:33:47] think about it this has never really
[00:33:49] happened in technology most of the time
[00:33:51] you build something big and expensive to
[00:33:54] build and valuable and then you give it
[00:33:56] to a small number of people first off
[00:33:57] the military and then it kind of makes
[00:33:59] its way down into like you know
[00:34:01] companies and consumers but we have this
[00:34:04] really awesome thing that is being given
[00:34:06] to everyone kind of immediately and
[00:34:07] that's great for consumers. There's a
[00:34:08] ton of consumer surplus if you want to
[00:34:10] be an economist. Um
[00:34:13] but then there's also open-source models
[00:34:16] now.
[00:34:16] >> Yep. uh which are actually by some
[00:34:18] measures probably more than 50 to 60% of
[00:34:21] calls within potentially applications
[00:34:23] according to some of the numbers. How
[00:34:24] how how is that the open source versus
[00:34:27] closed source in your opinion going to
[00:34:29] play out over the coming years?
[00:34:30] >> I we we will see um like I I'm kind of
[00:34:35] excited to see progress on both
[00:34:38] dimensions. like the the thing that I
[00:34:40] can't really square with the open source
[00:34:43] uh stuff is if we still are in the
[00:34:48] domain where we get serious returns on
[00:34:52] model capability from scale like I don't
[00:34:56] know what mechanism for open source
[00:34:59] model training is better than like a
[00:35:03] commercial model to like drive scale and
[00:35:05] so it's it's like a little bit different
[00:35:07] from you know getting Linux built open
[00:35:10] source. Like with Linux, you can have
[00:35:12] like a whole bunch of clever people who,
[00:35:15] you know, want to spend nights and
[00:35:16] weekends like making contributions to
[00:35:19] the the code base and like the whole
[00:35:22] ecosystem gets better as a function of
[00:35:24] like all of those contributions that are
[00:35:26] being made. But when you download and
[00:35:28] use the open source model, uh like
[00:35:31] you're not quite doing the same thing
[00:35:35] like you you certainly aren't uh like
[00:35:37] initiating a new round of pre-training
[00:35:41] or post- training uh on the model that
[00:35:43] goes back into the ecosystem that makes
[00:35:45] the like weights part of the model
[00:35:47] better. and like yeah there's uh some
[00:35:50] contributions that are happening in
[00:35:51] these open source models around the
[00:35:52] ecosystem and like a bunch of you know
[00:35:54] performance optimization work that's
[00:35:56] happening that's like I think really
[00:35:57] super valuable right
[00:35:59] >> um but again like I I think that scaling
[00:36:01] thing is the thing that uh you know you
[00:36:04] have to you have to think about but like
[00:36:06] the category error I think here is
[00:36:08] thinking that it's got to be either or
[00:36:11] I just don't see that at all um like we
[00:36:16] you know we and you Microsoft use both
[00:36:20] >> like we we have them available in our
[00:36:21] cloud and our own products like we're
[00:36:23] we're using like a mixture of things and
[00:36:25] so again going back to my earlier point
[00:36:28] like
[00:36:29] >> I I like worlds where we're fixated on
[00:36:32] like products and product outcomes and
[00:36:35] like you just sort of leave it to the
[00:36:36] engineers to sort out uh like what the
[00:36:39] infrastructure is going to look like and
[00:36:40] you know as long as the product's
[00:36:42] amazing it's getting better and cheaper
[00:36:43] and faster and like higher quality on
[00:36:45] all the dimensions that people care
[00:36:47] about like what do you care what the
[00:36:48] infrastructure is like whether it's like
[00:36:50] federation of open source models or you
[00:36:52] know like your L1 cache is like a open
[00:36:56] source thing that's running locally and
[00:36:58] like your you know your L2 is like a you
[00:37:00] know prompt that goes to a big expensive
[00:37:02] cloud model like whatever solves the
[00:37:04] problem right
[00:37:05] >> and again great news for kind of us as
[00:37:07] builders right like that you have this
[00:37:09] variety of different options that you're
[00:37:10] not actually locked in it's kind of an
[00:37:12] insane time to build from that dimension
[00:37:14] >> yeah and I look I love the open source
[00:37:15] models like I was uh I was just I still
[00:37:18] am like I like I've infinite amounts of
[00:37:21] curiosity so like being able to tinker
[00:37:23] around with stuff see how it works like
[00:37:24] I think they're just nothing but like
[00:37:26] positive.
[00:37:27] >> Yeah, absolutely. Maybe can we touch a
[00:37:29] little bit about data and this kind of
[00:37:32] pertains I think to open source in
[00:37:34] particular as well which is kind of
[00:37:37] understanding in some ways the the
[00:37:39] efficiency curves for data quality
[00:37:41] versus quantity. Yep. Especially kind of
[00:37:43] going forward. I think you've kind of uh
[00:37:46] emphasized that quality is obviously
[00:37:48] becoming more and more important and
[00:37:49] we're seeing this with all the companies
[00:37:51] that are kind of striving to create
[00:37:52] these specialized like environments for
[00:37:54] producing this data. But play this out
[00:37:57] over the next few years. What's your
[00:37:58] best kind of prediction in terms of
[00:38:00] where are both the big players going to
[00:38:02] continue to get their data? Um and where
[00:38:07] will perhaps the smaller players be
[00:38:09] getting the proprietary data? Oh, unique
[00:38:12] ways of getting data to train their
[00:38:14] their systems.
[00:38:15] >> Yeah, I I think we're it's it's pretty
[00:38:18] clear if you are like on on or near the
[00:38:23] frontier that for pre-training at least
[00:38:27] like we're well past the point where
[00:38:29] there are enough tokens organically
[00:38:32] occurring in the world to train bigger
[00:38:34] and bigger models. Uh and so it has been
[00:38:36] the case for quite a while now that like
[00:38:38] the super high capability uh like
[00:38:42] frontier models that are trained with a
[00:38:44] ton of compute have uh like a lot of
[00:38:47] synthetic uh tokens that they're uh that
[00:38:50] they're doing training on. Um but I like
[00:38:53] I also think it's like clear and clear
[00:38:55] to everyone that um you know having
[00:38:59] experts uh providing feedback in the
[00:39:02] post-training part of the process is uh
[00:39:06] like really critically important uh and
[00:39:09] that a lot of the you know this is a
[00:39:11] thing I would encourage you all to look
[00:39:13] at to the extent that you are um you
[00:39:16] know doing custom post trains on models
[00:39:18] at all yourself like you have to sort of
[00:39:20] think
[00:39:22] advantage uh still can exist now in like
[00:39:26] you understanding your domain and being
[00:39:28] able to identify experts that you can
[00:39:30] then use to you know as part of your
[00:39:33] post training process to make your
[00:39:35] particular post train for your
[00:39:37] application better. Um, so I think
[00:39:40] that's certainly a thing that we're
[00:39:42] going to uh continue to see and like
[00:39:44] we'll we'll see it in a whole bunch of
[00:39:46] ways. You know, like a bunch of this
[00:39:47] stuff uh if you can get to scale can
[00:39:50] happen with clever UI design inside of
[00:39:53] agents. Uh you know, like I was on uh
[00:39:57] Gemini the other day. Yes, I do use uh I
[00:39:59] do use my former employers uh product.
[00:40:01] >> It's a good model. Uh it it's a good
[00:40:04] model. Um but yeah, there's more good
[00:40:06] stuff coming too. Uh which is the really
[00:40:08] exciting thing. It's like this is uh you
[00:40:10] know something as well is like no one
[00:40:12] should uh no one should think that the
[00:40:14] next announcement that any one of us
[00:40:16] makes is the last great announcement
[00:40:17] that there's going to be uh be made. Uh
[00:40:20] just stuff's going to continue to grind
[00:40:22] and get better. But but you know but
[00:40:24] anyway like I you know I was in this uh
[00:40:27] I was using Gemini and I asked it like a
[00:40:29] technical question and it popped up to
[00:40:32] uh you know two completions for the
[00:40:34] prompt and asked me like which one I
[00:40:36] thought was better and so like you you
[00:40:38] will have like increasingly inventive UI
[00:40:41] treatments.
[00:40:42] >> That was cool. Yeah. The the generative
[00:40:44] UI stuff as part of Gemini was pretty
[00:40:46] cool.
[00:40:46] >> Yeah. Yeah. Yeah. But like the other
[00:40:47] thing too that's really important when
[00:40:49] you when you talk about data and like
[00:40:50] this may be the most important dimension
[00:40:52] of data is like you have the data that
[00:40:54] you're using in your pre and post
[00:40:56] training uh part of your um
[00:40:59] infrastructure building but you also
[00:41:01] have like all of the you know the stuff
[00:41:03] that we were talking about a minute ago
[00:41:04] which is like how do you hydrate memory
[00:41:06] like how do you make sure that you've
[00:41:07] like plumbed through all of the data
[00:41:09] sources that you want your models to
[00:41:10] have or your agents or AI systems to
[00:41:13] have access to in order to like actually
[00:41:15] solve problems. And so I think that
[00:41:17] maybe is the more important data. Uh and
[00:41:20] it's the place where you know you're
[00:41:23] going to have sort of the messier set of
[00:41:26] constraints, right? Because
[00:41:30] people who have
[00:41:32] proprietary data right now uh are on the
[00:41:37] one hand going to want to be able to use
[00:41:40] AI systems to get more value out of the
[00:41:43] proprietary data, but they're going to
[00:41:44] want to do it in a way that is uh not
[00:41:47] leaking a whole bunch of value out of
[00:41:49] their
[00:41:49] >> It's like value preserving for them.
[00:41:51] Yeah. Exactly.
[00:41:51] >> Yeah. And and so like that's an
[00:41:53] interesting set of problems to go solve.
[00:41:55] >> Yeah. One of the things that looking
[00:41:57] back, you know, one of the big um kind
[00:42:00] of mysteries, if you will, at least for
[00:42:02] me, for a long time, was it's kind of
[00:42:04] wild, right? Like the amount of
[00:42:05] pre-training that went into GPT35,
[00:42:07] right? What became chat GPT was
[00:42:09] enormous, but the amount of RLHF that
[00:42:12] kind of went into it was kind of tiny to
[00:42:14] kind of convert it from being a pretty
[00:42:16] good language model, uh, completion
[00:42:19] model to actually being an instruction
[00:42:20] following model, right? And the reason I
[00:42:22] bring this up is because my hypothesis
[00:42:25] would be that there might exist similar
[00:42:27] in some ways opportunities to be
[00:42:30] creative about small amounts of
[00:42:33] particular data kind of being able to
[00:42:36] tweak or kind of like shape the the
[00:42:39] performance of a model in ways that we
[00:42:41] just don't know yet. Right? So I think
[00:42:43] it's very easy to just be like you know
[00:42:45] to think about the only Laura adapters
[00:42:49] or like you know particular like
[00:42:50] supervised fine-tuning but if you're
[00:42:52] really kind of getting creative about I
[00:42:54] have access to this data set and then
[00:42:56] I'm going to try to get a model to do
[00:42:58] weird interesting things I suspect
[00:43:00] there's a lot of opportunity that we
[00:43:01] haven't yet fully explored uh that could
[00:43:04] actually be pretty novel. I think the
[00:43:06] chat GPT example is like maybe one of
[00:43:09] the most instructive things uh in the
[00:43:12] world for entrepreneurs right now. So
[00:43:14] the model that became the engine for
[00:43:17] chat GPT with that little bit of RHF
[00:43:20] was pretty old at the point where chat
[00:43:23] GPT had launched. And like there were a
[00:43:26] bunch of people including uh you know me
[00:43:29] and you know bunch of other people who
[00:43:33] had seen the model and like not a single
[00:43:37] one of us looked at this thing and said
[00:43:38] oh my god like this is going to be the
[00:43:40] next great consumer product that's going
[00:43:42] to potentially become a trillion dollar
[00:43:44] company. Um,
[00:43:48] there are these nuggets that are almost
[00:43:52] certainly that are out there right now
[00:43:54] that are extraordinarily valuable that
[00:43:56] if you just did the damned experiment to
[00:43:59] see whether like the value is there like
[00:44:02] will absolutely surprise all of us.
[00:44:05] >> Correct. Um and and so like that that's
[00:44:07] the thing I would just sort of encourage
[00:44:09] everybody especially right now like the
[00:44:11] the other thing that's happening with
[00:44:12] these AI systems is with all the you
[00:44:14] know the coding agent work that's
[00:44:16] happening and how fast that's getting
[00:44:18] better the cost of doing the experiments
[00:44:20] is never been cheaper
[00:44:21] >> 100%.
[00:44:22] >> So do the damned experiments try things.
[00:44:25] >> Yeah. and try to do things in my opinion
[00:44:28] that might actually just sound a little
[00:44:30] wacky or that maybe other folks aren't
[00:44:32] trying out because the the returns to
[00:44:36] them might actually just be kind of
[00:44:37] exponential like much like kind of
[00:44:39] converting the base pre-hat GPT model
[00:44:42] like with DLHF that was insane because
[00:44:44] and totally non-obvious I think.
[00:44:46] >> Yeah.
[00:44:46] >> Yeah.
[00:44:47] >> Yeah. Very I I don't think anybody would
[00:44:49] have anyone would have predicted that
[00:44:52] what happened
[00:44:53] >> was going to happen. Yeah.
[00:44:55] >> I mean the other thing too I would just
[00:44:57] really encourage folks not be precious
[00:45:01] about um yeah the possibility of failure
[00:45:05] because one of the things that one of
[00:45:07] the there's almost like this network
[00:45:08] effect that happens when like things get
[00:45:11] easyer
[00:45:13] and uh and then we go move faster in
[00:45:17] those uh you know along those vectors
[00:45:19] because the thing has gotten easier like
[00:45:21] we also build up more appetite for more
[00:45:24] people to try things. And so like I
[00:45:26] think you know we we have all-time high
[00:45:28] right now in the ease of running these
[00:45:30] experiments and I think you know
[00:45:31] all-time high in people's willingness to
[00:45:33] try new stuff. It's dizzying right now.
[00:45:35] Like I had a buddy who was the CTO of a
[00:45:39] game company that runs in six week
[00:45:41] sprints and he was like I went into one
[00:45:43] sprint uh like thinking that I was
[00:45:45] completely up to date on AI coding. I
[00:45:47] went heads down for six weeks to get
[00:45:49] this stuff done. I pull back up and it's
[00:45:50] like I feel like I know nothing now.
[00:45:52] like the entire world has changed in six
[00:45:55] flipping weeks. Uh and so, you know, I I
[00:45:59] just think the appetite for like trying
[00:46:01] all of this stuff that's changing is
[00:46:03] super high, which means like, you know,
[00:46:05] it's like almost this perfect uh you
[00:46:07] know, network of experiments and you
[00:46:10] know, and and I don't want to say
[00:46:12] experimentes, but you know, uh you know,
[00:46:15] folks who are willing to just look at
[00:46:17] anything to see whether it's worth
[00:46:19] something. I mean, I I love surfing
[00:46:21] myself and it's really just like right
[00:46:24] now it's like the wave is long. I'm kind
[00:46:27] of [ __ ] perfect and you just like get
[00:46:29] on that wave and you ride it for as long
[00:46:31] and you just kind of not have to worry
[00:46:32] about the other people surfing the wave
[00:46:34] because if you look back then you're
[00:46:36] going to trip over yourself. You don't
[00:46:37] want to look too far ahead because you
[00:46:39] can't really predict what the wave will
[00:46:40] do. But right now it's it's fun and you
[00:46:43] just kind of focus on your own form and
[00:46:45] you just kind of have some fun with it,
[00:46:46] right?
[00:46:46] >> Yeah. I I think that's a good analogy.
[00:46:49] Um, you know, so I have a my my eldest
[00:46:52] child is a 9-year-old and
[00:46:55] he recently started using kind of uh a
[00:46:58] bunch of like, you know, AI coding tools
[00:47:00] just to do V coding and he's kind of
[00:47:02] wipe coding his own games now. And
[00:47:06] his whole, you know, build, test, kind
[00:47:09] of like debug, deploy cycle is pretty
[00:47:12] interesting. So he doesn't type into the
[00:47:14] into cloud code. That's what he uses. He
[00:47:16] actually just draws pictures. He draws
[00:47:18] pictures of what he wants. He'll draw
[00:47:20] different game states and then he'll
[00:47:22] upload it. He'll understand it and then
[00:47:24] he'll just kind of chat with it and then
[00:47:26] eventually he'll draw something new to
[00:47:27] kind of get about it, right? And in the
[00:47:30] beginning I was like, what is this
[00:47:31] [ __ ] of? Yeah. Like why aren't you
[00:47:33] just like writing the thing? It takes
[00:47:34] you so long to draw this stuff. And over
[00:47:37] time I realized that ultimately this is
[00:47:39] just his own programming language,
[00:47:41] >> right? Like he has, you know, kind of
[00:47:42] come up with a UPL that he's feeling
[00:47:45] pretty good about. And I you know I
[00:47:48] think what's really interesting is that
[00:47:50] when I think about the moment in time is
[00:47:52] not only are the capabilities changing
[00:47:55] but also like a lot of the ways that we
[00:47:58] use this capability and everything we
[00:48:00] call vibe coding today is just kind of
[00:48:02] the you know what we thought of Python
[00:48:04] you know 15 years ago not a serious
[00:48:06] language you know like has a global
[00:48:08] interpreter lock what kind of serious
[00:48:09] programmer would use that and today it's
[00:48:11] kind of like you know uh
[00:48:14] >> I was chatting with Guido just last week
[00:48:16] about the uh about the gill.
[00:48:18] >> The Yeah, exactly. And so I we live in
[00:48:21] amazing times, but there's a question
[00:48:23] here, Kevin, which is that one of the
[00:48:24] things I think about pretty often is how
[00:48:27] do we prepare our kids for the age of AI
[00:48:30] and it's a very non-obvious question
[00:48:32] because the easy answers are that you
[00:48:35] want to give them agency. Absolutely.
[00:48:37] You have to make remind them that the AI
[00:48:39] is ultimately a tool that they use to
[00:48:41] express themselves and to build things.
[00:48:43] But I'm curious if you have any advice
[00:48:45] for my younger kids especially you know
[00:48:47] what three and five about they will live
[00:48:49] they will grow up in a pretty different
[00:48:50] world that I think any of us grew up in.
[00:48:53] Um so what what what advice would you
[00:48:55] give me for them?
[00:48:56] >> Well look I think the two things that
[00:48:58] you uh that you said are like good that
[00:49:01] you know they need to have agency and
[00:49:03] they need to think of the thing as a
[00:49:05] tool that's there to help them tackle
[00:49:07] the things that they think are
[00:49:09] important. Um, I, you know, I think the
[00:49:12] question to ask is like whether or not
[00:49:14] the AI systems are making people feel
[00:49:18] more or less empowered. Um, and so for
[00:49:22] your for your kiddo, I mean, at 9 years
[00:49:24] old, like it almost certainly has to be
[00:49:28] empowering to be able to like draw a
[00:49:30] video game to existence. And so, you
[00:49:32] know, in a certain sense, it's like who
[00:49:34] cares what the mechanism is? like you
[00:49:36] know they they are accomplishing a thing
[00:49:38] and they
[00:49:39] >> they are learning early uh to be
[00:49:42] fearless about how it and so like I
[00:49:44] think kids in a certain sense are going
[00:49:46] to have like the same advantages that
[00:49:48] kids always have when they're sort of
[00:49:50] like coming of age when a new technology
[00:49:52] is uh you know is is emerging really
[00:49:55] really quickly because like they just
[00:49:56] won't be afraid of using it in ambitious
[00:49:59] ways and they won't have a whole bunch
[00:50:00] of preconceived notions that are
[00:50:02] constraining them in how they think
[00:50:03] about using
[00:50:05] Um, but yeah, I mean I I think a lot of
[00:50:09] it is going to be getting back to
[00:50:10] fundamentals. It's going to be, you
[00:50:12] know, do you have good taste about
[00:50:13] problem selection? Like do you really
[00:50:15] understand like, you know, how systems
[00:50:18] work and you fit together? Like do you
[00:50:21] like are you really thinking about what
[00:50:23] you're doing in a serviceoriented way?
[00:50:26] like what am I doing with these tools
[00:50:28] that is of service and of value to my
[00:50:30] fellow human beings uh versus just you
[00:50:34] know screwing around uh you know inside
[00:50:36] of the system for the sake of you know
[00:50:38] like it's a video game or something
[00:50:39] right you know and I think that the
[00:50:41] things that are true for kids are true
[00:50:42] for all of us like there are certain uh
[00:50:45] certain things for sure that are going
[00:50:47] to change in pretty dramatic ways but
[00:50:50] like I think everything that's changing
[00:50:52] is also presenting a set of like super
[00:50:54] interesting opportunities for people
[00:50:56] because like uh there is a positive sum
[00:51:00] empowerment mechanism that's uh that's
[00:51:03] at work here. Um, I mean like the the my
[00:51:07] 17-year-old is a bio nerd and you know I
[00:51:10] remember she asked me a few years ago
[00:51:12] was like
[00:51:13] >> oh do you think uh you know being a
[00:51:15] heart surgeon is uh you know is a stable
[00:51:18] job giving all the AI and I was like
[00:51:20] yeah almost for certain like you know
[00:51:21] the population's getting older like you
[00:51:23] know heart disease isn't going to get
[00:51:24] cured uh anytime soon and like you know
[00:51:28] even though the robotics is getting a
[00:51:29] lot better like a huge amount of
[00:51:31] medicine is about uh you know human
[00:51:33] human contact and like you know just a
[00:51:36] bunch of super messy stuff that you
[00:51:38] can't solve entirely with technology and
[00:51:40] people don't want solved entirely with
[00:51:43] technology. Um, and so I think, you
[00:51:45] know, those are things too for kids that
[00:51:47] they ought to be looking for in terms of
[00:51:50] careers. Like anything that you can sort
[00:51:51] of look at and say, "Oh my god, like
[00:51:53] this is and all of us have this and to
[00:51:55] some degree inside of our jobs is
[00:51:58] robotic uh and repetitious." Like those
[00:52:01] are things that some technology whether
[00:52:03] it was AI or not was going to come get
[00:52:05] at some point because you've got the
[00:52:07] dual problem of this thing is irritating
[00:52:09] to me uh doing the work and so you know
[00:52:12] it's like likely also
[00:52:14] >> irritating to other people like Yeah.
[00:52:15] Exactly. Yeah. Yeah. Yeah. Um
[00:52:18] amazing. Well, this kind of brings us to
[00:52:21] time. I think that uh thank you so much
[00:52:23] Kevin for coming by and kind of sharing
[00:52:25] some highlights with us. Um any final
[00:52:29] thoughts for this group here?
[00:52:30] >> You all are
[00:52:33] maybe doing the most fun thing in the
[00:52:35] world right now, which is uh starting
[00:52:38] companies and building things at the
[00:52:40] best [ __ ] time that I have ever seen
[00:52:42] in my career to be
[00:52:44] >> it really is
[00:52:45] >> building things. So uh and I I know
[00:52:49] building things is hard. It's a grind.
[00:52:51] Just don't lose sight of like how
[00:52:54] special this moment is. uh like I think
[00:52:56] everybody in this room has the potential
[00:52:59] to make a massive impact on the world uh
[00:53:03] by just being fearless about how you put
[00:53:08] this technology at work for other
[00:53:10] people.
[00:53:11] >> Can I can I perhaps end with a question
[00:53:13] if that's okay?
[00:53:14] >> Sure. Um, you know, one of the things
[00:53:16] that I often talk to founders about is
[00:53:18] that there is this feeling that if they
[00:53:21] solve this one thing, raise this next
[00:53:23] round, then it all the [ __ ] becomes
[00:53:25] easier. And I often tell them that like
[00:53:28] no, Kevin's playing the same game as you
[00:53:30] are. Sure, some of the scale might be
[00:53:32] different, but like ultimately like if
[00:53:34] you are kind of in the mode of like I
[00:53:37] want to build cool new things, I will
[00:53:39] push myself to the limit. then it will
[00:53:41] be hard it will be challenging right so
[00:53:44] I think it's just whenever I you know
[00:53:46] when whenever a founder is kind of like
[00:53:48] oh if I just get to this next thing
[00:53:50] it'll become easier I'm like it doesn't
[00:53:51] >> it won't
[00:53:52] >> it just doesn't like you have to enjoy
[00:53:54] the game along the way and also
[00:53:56] acknowledge that the like if you're
[00:53:57] playing the game like you know
[00:53:59] competitively it's supposed to be hard
[00:54:00] you know yeah
[00:54:01] >> so yeah
[00:54:02] >> I've I've always described myself as a
[00:54:05] short-term pessimist long-term optimist
[00:54:08] so my short-term pessimism is like
[00:54:09] everything's [ __ ] uh like just it's
[00:54:12] like it's just awful. And you know and
[00:54:14] like I and I carry myself through the
[00:54:16] day uh you know with with that uh you
[00:54:19] know sort of spirit. Um and I had a boss
[00:54:22] uh like I don't think you'll mind me
[00:54:25] saying this. So Jeff Weiner who was the
[00:54:27] uh CEO of LinkedIn who is a dear friend
[00:54:29] and like I learned so much from him. I
[00:54:31] had a one-on-one with him one time where
[00:54:33] he looks at this grumpy [ __ ] piece of
[00:54:35] [ __ ] engineer who works for him and he's
[00:54:36] like dude you're always unhappy. I'm
[00:54:38] going to help you reset your hydonic
[00:54:40] equilibrium. And I was like, the [ __ ]
[00:54:42] are you even talking about, man? It's
[00:54:44] like, I don't want my hideonic
[00:54:45] equilibrium set. It's like, I don't even
[00:54:47] want to be happy. Like, that's not the
[00:54:48] first order thing. I want to do
[00:54:50] meaningful work. Meaningful work is
[00:54:52] hard. And I'm not going to be happy
[00:54:54] while I'm doing it, but I will be
[00:54:56] content. And like, I think that's all I
[00:54:58] need to hope for in my life is be
[00:55:00] content at doing meaningful things.
[00:55:02] Yeah. It's all going to be hard, right?
[00:55:05] learn to like accept and enjoy and
[00:55:08] appreciate the hardness. It is a
[00:55:10] privilege that you all get to solve hard
[00:55:12] problems.
[00:55:13] >> 100%
[00:55:14] >> privilege.
[00:55:14] >> No, could not agree more. I I don't
[00:55:16] think I have anything that can end
[00:55:17] better than that. So, thank you so much,
[00:55:19] Ev. Thank you for coming.
[00:55:22] >> That was another episode of Minus One
[00:55:24] from the team at South Park Commons.
[00:55:26] Make sure to subscribe to our show
[00:55:28] wherever you listen to podcasts and find
[00:55:30] us on social at South Park Commons. And
[00:55:33] thanks to our friends at Atomic Growth
[00:55:35] for their support in bringing this
[00:55:37] episode to
