[00:00:00] Eight months ago, it was just kind of
[00:00:02] debugging SQL. Now AI can really start
[00:00:04] to write that initial boilerplate SQL
[00:00:07] query and code that helps you then build
[00:00:09] on to do more advanced analyses. I've
[00:00:12] set those dashboards up many times in my
[00:00:14] career and they end up like kind of just
[00:00:15] getting ignored at some point. [music]
[00:00:17] Cloud will always read it. Cloud will
[00:00:19] always kind of start to ask questions.
[00:00:21] Recreating the way a data scientist
[00:00:23] actually thinks inside of a company.
[00:00:25] They're not just thinking of only the
[00:00:26] data. They're thinking of all the things
[00:00:27] outside of that. Podco is the first tool
[00:00:30] that I feel like could actually
[00:00:30] accomplish that.
[00:00:31] >> Which AI coding tool has the most
[00:00:33] momentum? Which one's gonna win?
[00:00:35] >> The customers on our platform have
[00:00:36] already picked it is definitely cursor.
[00:00:38] Cursor is absolutely kind of brushing it
[00:00:41] in startup coding tool of choice, but
[00:00:43] also enterprise coding tool of choice.
[00:00:46] >> So before we get into the cloud host
[00:00:48] stuff, can you talk about at a high
[00:00:49] level how AI can help with like typical
[00:00:51] data tasks? So yeah, here's kind of like
[00:00:54] the high level view of what I would say
[00:00:56] a typical analyst does in their their
[00:00:59] role and really how cloud code can start
[00:01:01] to augment that. You know, I could kind
[00:01:04] of think super high level and think
[00:01:05] about what kind of you would do from
[00:01:08] starting from scratch, but the reality
[00:01:09] is most people in their current job,
[00:01:11] they're not starting from scratch. They
[00:01:12] already have a series of dashboards.
[00:01:15] They have a series of queries they're
[00:01:16] running. And so at a very kind of
[00:01:19] starting point, it's actually just
[00:01:21] monitoring those things, right? And so
[00:01:22] Claude can actually like run your
[00:01:24] queries for you and and look at them and
[00:01:26] understand what the trends are and
[00:01:28] summarize them in a way that you're
[00:01:30] probably if you're you're in a role
[00:01:32] inside of a company like checking those
[00:01:34] four or five dashboards um and
[00:01:37] constantly trying to understand, okay,
[00:01:38] is this trend actually a meaningful
[00:01:41] change? Is it something I need to
[00:01:42] investigate? Um because the LLM's can
[00:01:45] actually like interpret those things
[00:01:47] themselves, it can actually help you
[00:01:49] scale that to monitor more than you
[00:01:51] might typically choose to monitor by
[00:01:53] spending hours by looking through each
[00:01:55] kind of dashboard and cell. It can
[00:01:57] actually do that for you. And I think
[00:01:59] that's been one of the first kind of
[00:02:00] real kind of game changers when it comes
[00:02:03] to folks building their own little
[00:02:05] monitoring agents uh and dashboards
[00:02:07] within cla and then you know once you
[00:02:10] see that trend and think about you know
[00:02:12] I probably do want to investigate this
[00:02:16] claude does a great job of building out
[00:02:18] context either through the actual data
[00:02:21] itself but also what's going on in Slack
[00:02:24] what's going on in um these other tools
[00:02:26] like linear and tickets that are kind of
[00:02:28] like being created around that
[00:02:30] particular topic and cloud can access
[00:02:31] all that which is honestly the job of a
[00:02:34] data person right you might want to
[00:02:35] actually like dive into and actually do
[00:02:37] that work in advance propose hey it
[00:02:40] looks like one of your metrics have
[00:02:42] changed let's go into the customer level
[00:02:44] let's go into the transaction level and
[00:02:46] understand is there something out of the
[00:02:49] norm happening in those those trends so
[00:02:52] that exploration phase is actually
[00:02:53] really powerful um if you already kind
[00:02:56] of know what you're monitoring and what
[00:02:58] you want to track, you could jump right
[00:02:59] into number two and actually be really
[00:03:01] really helpful.
[00:03:02] >> And then also uh crafting a story which
[00:03:04] not not all data scientists are good
[00:03:05] good at, you know. Yeah, this one is,
[00:03:07] you know, where the it starts to kind of
[00:03:09] drift from something Claude can do
[00:03:10] perfectly to where Claude really kind of
[00:03:13] getting help and kind of guide folks
[00:03:16] where with the craft part um trying to
[00:03:19] actually tell a story that changes
[00:03:21] people's minds that truly like proposes
[00:03:24] something that makes sense to the
[00:03:26] business is where you really have to
[00:03:28] kind of use your uh you know prompting
[00:03:31] to actually give it this like ability to
[00:03:34] translate back to you what a good story
[00:03:37] is. And so what's great is that it can
[00:03:38] kind of do those side investigations,
[00:03:40] those deep dives. As you start to craft
[00:03:42] a story, you might realize there are
[00:03:44] kind of things you want to investigate
[00:03:45] and add to, which ends up being, you
[00:03:47] know, in reality a bunch of comments
[00:03:49] inside of Google Doc. But you can
[00:03:50] actually take those and have kind of
[00:03:52] claude work on those in a pretty quick
[00:03:54] manner to hopefully fill in any gaps as
[00:03:55] you start to layer in this story of what
[00:03:57] happened. And what's great also is that
[00:04:00] as you start to like propose certain
[00:04:02] changes, you can start to like look
[00:04:04] through the codebase, look through pre
[00:04:07] experiments, through things that have
[00:04:09] been done to change something similar
[00:04:11] and actually start to size what the
[00:04:12] potential impact is.
[00:04:14] >> Awesome. Uh and the last one is just
[00:04:16] about impact, right?
[00:04:17] >> Yeah.
[00:04:17] >> Yeah. The impact piece is this is what
[00:04:20] the one thing like I haven't quite seen
[00:04:22] cla go from end to end in this way that
[00:04:25] I know is possible, right? as kind of
[00:04:28] each of these different steps starts to
[00:04:29] improve like you can imagine the world
[00:04:32] where you set up kind of claw to do this
[00:04:35] analysis it understands the code base
[00:04:37] understands how things have changed in
[00:04:39] the past and actually goes in and
[00:04:41] changes a particular button you know in
[00:04:43] your app to be in a different order then
[00:04:46] runs an experiment on that and then
[00:04:48] starts this whole cycle again to
[00:04:50] understand how that experiment has been
[00:04:52] successful or not successful why that's
[00:04:54] not the case how to tell the story and
[00:04:56] what to change and then propose again
[00:04:58] and actually implement those change. So
[00:05:00] this loop of analysis is something that
[00:05:02] every data scientist is working through
[00:05:04] alongside the product managers and
[00:05:06] engineers and I've already seen clot
[00:05:09] start to augment each one of these uh
[00:05:11] and I think we all can understand there
[00:05:13] is a future here where it can actually
[00:05:15] do all of them um in sequence and kind
[00:05:18] of continuously do that to actually be
[00:05:21] this like endtoend product manager, data
[00:05:23] scientist engineer all at once. Yeah,
[00:05:26] I've worked with like a quite a few data
[00:05:27] scientists in my career and um there's
[00:05:29] like a bunch of stuff that they don't
[00:05:31] actually want to do, right? Like they
[00:05:32] don't want to write SQL manually. They
[00:05:33] don't want to build dashboards. It sucks
[00:05:36] [laughter] dashboards.
[00:05:37] >> Uh they don't want to like they want to
[00:05:39] do like deeper analysis and deeper
[00:05:41] thinking but like a lot of the work is
[00:05:42] just like building dashboards and like
[00:05:44] you know the PM pay them like hey can
[00:05:45] you run this random query?
[00:05:48] So, so yeah, if they can use cloud to,
[00:05:50] you know, try to do a lot of that work
[00:05:52] or at least do it faster, like that
[00:05:53] would help hopefully increase job
[00:05:55] satisfaction a lot too.
[00:05:56] >> Yeah, job satisfaction, but also the PM
[00:05:59] satisfaction of being able to get those
[00:06:00] numbers without having to talk to a data
[00:06:02] person to go get it. I think that's one
[00:06:04] of the examples talking to the team and
[00:06:07] product managers uh here at B seen folks
[00:06:10] just take a dashboard that exists take
[00:06:13] all the queries throw it into cloud code
[00:06:15] have it run every single day or week
[00:06:17] whenever they're trying to check on kind
[00:06:19] of what's going on in the monitoring
[00:06:21] step here and do their own explorations
[00:06:23] do their own investigations around
[00:06:25] what's happening and
[00:06:26] >> kind of cuts out the data person which
[00:06:28] maybe isn't you know great for job
[00:06:30] satisfaction but what it does do is have
[00:06:32] them ask better questions Right. And
[00:06:34] that that leads to that work of actually
[00:06:36] having time to do better analysis,
[00:06:39] deeper thinking around how to actually
[00:06:41] improve the product. And that's been
[00:06:43] really interesting to me to see that
[00:06:45] stuff start to actually happen here.
[00:06:47] Let's take a quick example. So like um I
[00:06:49] think most companies have um like you
[00:06:52] know every Monday or something or like
[00:06:54] every Friday uh there's like a superet
[00:06:56] or something that like generates a
[00:06:57] chart, right? Or like send sends you an
[00:06:59] email with a bunch of charts. But like
[00:07:01] uh with cloud and you know AI, how is
[00:07:03] that better now? So instead of just
[00:07:04] sending you a dashboard, it will
[00:07:06] actually do a bunch of insight analysis.
[00:07:08] >> Yeah, I think there's two parts that
[00:07:10] make it better is you can kind of
[00:07:11] customize it really easily, right? You
[00:07:13] can set up this dashboard to be what
[00:07:15] maybe a team needs, but maybe there's
[00:07:18] folks on the team that only want to know
[00:07:19] certain things or really customize it to
[00:07:22] be about one type of the users that are
[00:07:25] using particular product or feature. Um,
[00:07:28] and Claude can quickly modify, change
[00:07:31] that, make it very customized to that
[00:07:33] person's role that what they care about.
[00:07:35] So, that's one of the things that I've
[00:07:36] seen do it do really well. Um, and then
[00:07:40] additionally, it can actually like run
[00:07:42] that analysis, do the follow-up ask or
[00:07:44] at least generate the questions on what
[00:07:47] I think happens when those emails go
[00:07:48] out. It's like, okay, some people read
[00:07:50] them, some people really read them, come
[00:07:53] up with like all the questions. Cloud
[00:07:55] will always read it. Claude will always
[00:07:57] kind of start to ask questions and then
[00:07:59] kind of you can start to see oh like can
[00:08:03] I actually have Claude answer these
[00:08:04] right and it'll start to try to answer
[00:08:06] it too and I think that kind of you know
[00:08:09] ability to always know that someone is
[00:08:11] reading it is part of what's important
[00:08:12] because I' I've set those dashboards up
[00:08:14] many times in my career and they end up
[00:08:17] like kind of just getting ignored at
[00:08:18] some point by a group of people that are
[00:08:20] getting it because they felt like they
[00:08:21] should have got it but they actually end
[00:08:22] up don't. Um I don't think Claude ever
[00:08:24] feels that way. So there's that kind of
[00:08:26] like feeling that I've seen a lot where
[00:08:29] it actually means someone will read it
[00:08:30] even if it's just Claude.
[00:08:32] >> Got it. And also like like your slide,
[00:08:34] the fact that it has access to more than
[00:08:36] just the data warehouse. It has like the
[00:08:38] Slack access and a bunch of other
[00:08:39] contexts hopefully will help it come
[00:08:41] with better insights, right?
[00:08:43] >> Yeah, that's that was huge. Like for
[00:08:45] example, we had kind of an incident uh
[00:08:47] here and it was affecting some of our
[00:08:49] metrics and the metrics that were
[00:08:50] running in this weekly review started to
[00:08:52] kind of change. it was able to
[00:08:54] understand there's an incident happening
[00:08:56] at that specific time uh and that
[00:08:59] impacted the metrics right it was a data
[00:09:01] incident so it impact the actual end
[00:09:03] customer but it definitely impacted why
[00:09:04] these metrics look wrong but it it
[00:09:06] searched for that um in our Slack uh I
[00:09:10] was able to see there's some fixes
[00:09:12] ongoing some tickets being created to
[00:09:14] resolve that um so it saved me like what
[00:09:17] have would have been like wait what's
[00:09:19] happening to the data like [laughter] I
[00:09:21] had to go look at specific customers
[00:09:22] maybe you kind you know, leaving, but
[00:09:24] turns out there was actually a real
[00:09:27] reason for it on a Monday morning that
[00:09:30] >> got it. Got it.
[00:09:30] >> Started, you know, me the process of
[00:09:32] fixing that incident. But still, at
[00:09:34] least um Claude can understand that
[00:09:36] pretty quickly.
[00:09:38] >> Yeah, that's awesome, man. Yeah, I I
[00:09:40] feel like most companies don't have that
[00:09:41] set up at all. Like, so so yeah, it's
[00:09:44] great that you and are doing doing this.
[00:09:46] >> Well, I think most companies have like a
[00:09:47] data person who's kind of like just
[00:09:49] doing that, right, all the time, reading
[00:09:50] the these things, searching, right? Um,
[00:09:52] but now you can kind of help that person
[00:09:54] do this.
[00:09:55] >> Why don't we make this practical? Like
[00:09:57] maybe you can show us how this stuff
[00:09:58] actually works in CL code. Maybe like
[00:10:00] let's let's take like a public data set
[00:10:03] or something to look at.
[00:10:04] >> Yeah, cool. Let's look at kind of the
[00:10:06] startup funding round data sets which
[00:10:09] kind of mimics Brexit's data because
[00:10:11] they're just transactions, right? Money
[00:10:12] being moved from one venture capitalist
[00:10:14] to a startup. And so that's why I'm kind
[00:10:16] of picking that one. And um I'll kind of
[00:10:20] go into like how I set it up. Um some of
[00:10:23] the pro potential problems that comes
[00:10:24] with like doing aentic analytics and how
[00:10:27] you can kind of address that in the
[00:10:28] setup process. Um and then try to
[00:10:31] actually show you some of these use
[00:10:33] cases. How does that sound?
[00:10:34] >> Sure. Yeah, that sounds great.
[00:10:36] >> Um all right, let's go into it. Let's
[00:10:38] kind of start here. Um, so the first
[00:10:41] thing I would kind of think about is
[00:10:43] actually like what is the data set that
[00:10:46] you're trying to build kind of an MC
[00:10:49] what I'm kind of kind of using here
[00:10:50] isn't the MCP framework. Um, and I think
[00:10:54] the part that you really can start with
[00:10:56] it's it's pretty simple. You don't have
[00:10:58] to get too complicated in how you think
[00:11:00] about like give the claude code three
[00:11:04] queries around the metrics in a specific
[00:11:07] domain within your company and actually
[00:11:10] start to instruct it to build an MCP
[00:11:12] around that like that. That's that's it.
[00:11:13] That's all you really need to start up
[00:11:16] kind of what I would call is like a
[00:11:18] pretty basic MCP. Now, as you add in
[00:11:20] more domains, things start to get a
[00:11:21] little bit more complicated, but for us,
[00:11:23] like that was kind of the initial
[00:11:25] formation around trying to start this.
[00:11:27] And so um thinking through this data set
[00:11:30] here I'll open up um the three queries I
[00:11:34] have and so you can just kind of look at
[00:11:35] them. We have kind of using this we call
[00:11:38] it market data inside of brack these
[00:11:40] funding rounds. We're just kind of
[00:11:42] looking at some basic information around
[00:11:44] the first one here being monthly startup
[00:11:46] funding trends by industry and stage. Um
[00:11:50] they're kind of showing the kind of
[00:11:52] cloud code how to write these queries,
[00:11:54] what a typical join would be. A lot of
[00:11:56] it's based off domain name. Uh so that's
[00:11:59] kind of something that's being
[00:12:00] established here. We're adding some
[00:12:02] documentation around what this you know
[00:12:04] table kind of has inside of it. Um you
[00:12:08] know why there's two tables, right? One
[00:12:10] is the funding rounds and one has the
[00:12:11] actual startups. uh along with how are
[00:12:14] these kind of funding rounds like named
[00:12:17] series_a helps kind of give it that
[00:12:19] guidance later on uh we added a second
[00:12:22] query to think about the other set of
[00:12:24] data in here which are the investors so
[00:12:26] I'm kind of trying to build two to three
[00:12:28] queries that have each of the tables at
[00:12:32] least one join together um to kind of
[00:12:35] show cloud code some of these patterns
[00:12:37] and some of the key fields inside of the
[00:12:40] codebase and so kind of looking at that
[00:12:42] trying to rank the top investors trying
[00:12:44] to add something around kind of when how
[00:12:46] often they're funding these customers in
[00:12:49] their most recent years. Um and actually
[00:12:52] here's the third query which we have the
[00:12:54] startup ecosystem healthcore. So this is
[00:12:56] one where you might actually have some
[00:12:58] analysis examples. So like two basic
[00:13:00] ones to really illustrate um how to
[00:13:04] build some of the metrics within the
[00:13:06] data set and then I'd say one to try to
[00:13:08] show how you might actually think about
[00:13:10] doing something a little bit more
[00:13:11] complicated um something that's actually
[00:13:14] like trying to understand the velocity
[00:13:16] of funding that a particular you know
[00:13:19] startup ecosystem um is seeing. So
[00:13:23] that's something that I found to be
[00:13:24] really a kind of a good uh set of kind
[00:13:27] of criteria for starting this off.
[00:13:30] >> Okay. So you wrote these queries
[00:13:31] manually or maybe with some AI's help.
[00:13:34] >> So there's actually like you can kind of
[00:13:35] reverse engineer them. You can actually
[00:13:37] go in and search for these tables and
[00:13:40] see what are the queries being run
[00:13:41] inside your Snowflake instance um and
[00:13:43] start to pick from there. So that was
[00:13:45] that bit of how I started with these
[00:13:46] because I didn't necessarily want to
[00:13:47] like sit down and just create queries
[00:13:49] that are being used because these tables
[00:13:52] already existed. I was able to really
[00:13:54] just like scrape them off of what was um
[00:13:58] already being run in a dashboard or in
[00:14:00] our Snowflake um instance and then kind
[00:14:03] of start to like actually ask Claude to
[00:14:05] document some of them uh which you know
[00:14:07] doesn't always happen uh in the actual
[00:14:09] like running of the queries. So that
[00:14:12] helped kind of layer in some of this. So
[00:14:14] the goal here is just to maybe give Claw
[00:14:16] some understanding of how to run queries
[00:14:17] for these different tables. That's kind
[00:14:19] of
[00:14:20] >> exactly kind of give it those rails. And
[00:14:22] so here's kind of where I would start to
[00:14:25] actually write um kind of the prompt to
[00:14:29] actually generate the startup MCP. Um we
[00:14:32] actually kind of go in and say, you
[00:14:34] know, create a startup funding
[00:14:40] MCP. Um, and I would say don't use the
[00:14:45] existing one.
[00:14:48] Um, that uses
[00:14:51] these
[00:14:53] three queries.
[00:14:55] And I think I would try to kind of also
[00:14:59] instruct it to do some research, do some
[00:15:02] research online about ways to set up a
[00:15:06] great MCP. Um I I probably also want to
[00:15:10] give it kind of like a rough idea of
[00:15:13] what like these data sets are. Um so
[00:15:17] this data set is
[00:15:21] um a overview
[00:15:25] of the funding rounds uh for startups
[00:15:29] and includes the investors
[00:15:33] uh transactions and startup details.
[00:15:38] Uh I also kind of want to tell a little
[00:15:40] bit about kind of why we might want to
[00:15:42] use this. We want to use this to
[00:15:47] understand what types of new funding
[00:15:53] rounds are happening amongst
[00:15:57] our customer set so we can you know
[00:16:01] engage with them. So it's obviously
[00:16:03] really valuable to us when we understand
[00:16:05] someone's getting a bunch of funding to
[00:16:07] like engage with them and make sure that
[00:16:08] they are happy on brides. I think I kind
[00:16:11] of also think about giving it some
[00:16:14] instruction around kind of why I want to
[00:16:17] use the MCP framework as well. So like
[00:16:20] um we want to build an MCP that allows
[00:16:28] us to build weekly recurring reporting.
[00:16:34] do deep dive analysis and do a quick
[00:16:38] kind of deep dive analysis and help
[00:16:41] monitor trends
[00:16:44] within startup.
[00:16:47] It's kind of it. I think I I also like
[00:16:50] always like to kind of have it think of
[00:16:53] trying to do an eval set. Um, and then
[00:16:56] like if you kind of kind of set it up
[00:16:58] that way, you can then later on, you
[00:17:00] know, edit and understand what the eval
[00:17:02] might look like and change that. So here
[00:17:04] I would actually um say create three
[00:17:08] eval
[00:17:10] questions
[00:17:12] to test the MCP at the end. And I think
[00:17:16] in this case also because it's a startup
[00:17:18] funding data set there's some questions
[00:17:20] it might try to think of that are
[00:17:22] actually kind of correct not too domain
[00:17:24] specific to bracks. Um so feeling kind
[00:17:28] of confident there. So we'll kind of uh
[00:17:30] obviously go into planning mode because
[00:17:32] that will actually help us um see how
[00:17:35] it's going to process and understand
[00:17:36] this um and we kind of see what the plan
[00:17:39] looks like and edit it. just for the
[00:17:41] people who don't understand like why why
[00:17:43] uh why make an MCP for this task.
[00:17:46] >> Yeah. Um
[00:17:47] >> yeah.
[00:17:48] >> Yeah. There's a bunch of I think the the
[00:17:50] core reason I think about why an MCP is
[00:17:53] you want to be able to have Claude
[00:17:55] access your data in um a structured way
[00:17:59] that is somewhat repeatable across folks
[00:18:02] using claude to do this types of
[00:18:03] analysis. So without the MCP, you could
[00:18:06] think of, you know, someone tossing in a
[00:18:08] query and telling claw to answer a
[00:18:10] question about startup funding rounds,
[00:18:12] it's not going to understand there is,
[00:18:15] you know, a way to do that. There's some
[00:18:17] documentation that exists that's been
[00:18:18] kind of developed by the data team and
[00:18:20] kind of um the folks that have been kind
[00:18:24] of writing these queries. And so to me,
[00:18:26] that's one of the values here is the MCP
[00:18:30] kind of starts to build those rails so
[00:18:32] that anyone who tries to access this
[00:18:34] data can start in the right place. I
[00:18:36] think we always understand that there
[00:18:38] might be modifications outside of that
[00:18:40] initial starting place, but so far I've
[00:18:42] seen the MCP kind of framework help
[00:18:45] connect to Snowflake in a consistent way
[00:18:48] to write queries more consistently. Um,
[00:18:52] >> got it. and actually like enable kind of
[00:18:55] that analysis uh step.
[00:18:57] >> Okay. So yeah, basically they don't have
[00:18:58] to do all this setup each time if you
[00:19:01] create MCP. Yeah,
[00:19:02] >> exactly.
[00:19:03] >> Makes sense.
[00:19:04] >> So it's asking me some questions here
[00:19:05] like should I use all four queries? I
[00:19:08] mentioned three. Um so kind of do you
[00:19:12] want to deploy it to production ready? I
[00:19:14] think I'm just going to say locally need
[00:19:17] production ready. um saying I should
[00:19:19] have some alerting so I can actually
[00:19:21] develop that. Um I don't think I'll do
[00:19:24] any alerting in this step. Just kind of
[00:19:27] skip that. Um and then here's kind of
[00:19:31] the answers themselves. So I always love
[00:19:34] to have these kind of like unanswered
[00:19:35] questions come back. Kind of makes me
[00:19:37] feel more confident about the plan
[00:19:40] >> um that's coming.
[00:19:41] >> Yeah, this is like a this is like a new
[00:19:42] feature, right? I don't think I used to
[00:19:43] ask all those questions before. Uh yeah,
[00:19:45] I did set it up uh in my uh user
[00:19:49] >> Oh, yeah. Cloud MD.
[00:19:50] >> Yeah, cloud MD to always ask me those.
[00:19:53] So, I'm not sure if it's new, but yeah,
[00:19:55] I definitely saw someone give that tip.
[00:19:57] Maybe it was on your uh pod and I kind
[00:20:00] of have added it's been really useful.
[00:20:02] >> Nice.
[00:20:03] >> You might even ask even more to be
[00:20:05] honest.
[00:20:06] >> And sorry, this maybe is a dumb
[00:20:07] question, but like how is this thing how
[00:20:08] is this project that you have the Summit
[00:20:10] OS connected to Astro database? It's it
[00:20:14] just you connected before or
[00:20:16] >> I didn't cover that. Yeah. Uh basic I
[00:20:18] set up the kind of Snowflake CLI. Um so
[00:20:22] it actually has access via that and then
[00:20:24] to like ensure it's a little bit more
[00:20:26] reliable added in the Snowflake pat
[00:20:28] token and
[00:20:29] >> kind of added that to my cloud. So it's
[00:20:32] it's able to access the the Snowflake
[00:20:34] instance to uh view those things.
[00:20:37] >> All right, man. You asked it do a lot.
[00:20:38] So it's got a big list of tasks to do
[00:20:40] now.
[00:20:40] >> I [laughter] know. Yeah, it actually
[00:20:42] added a little more to than what I
[00:20:44] thought. But um
[00:20:45] >> yeah,
[00:20:45] >> we'll see kind of how it progresses and
[00:20:48] I'm happy to talk while it's building um
[00:20:50] because I did have it bypress
[00:20:52] permissions on some of like the
[00:20:53] potential issues that you can come
[00:20:55] through come up uh that I'm happy to
[00:20:58] talk about some of the issues that come
[00:20:59] up when you actually do analysis with
[00:21:02] AI. We usually kind of feed that back
[00:21:04] into the MCP and have it kind of address
[00:21:07] some of those issues.
[00:21:08] >> Yeah. Should we dive into it?
[00:21:09] >> Sure. Yeah.
[00:21:10] >> So, when it comes to like problems that
[00:21:13] I've seen doing a lot of analysis with
[00:21:16] Claude, um there's some specific issues
[00:21:20] that I think are different than how you
[00:21:22] might want to use Claude to write a doc
[00:21:25] or um change the codebase. And it comes
[00:21:28] to the fact that when you actually write
[00:21:30] a query, it might return 10,000 rows or
[00:21:33] two million rows, right? Actually
[00:21:35] blowing up your entire context window.
[00:21:37] Um, that's unlike a like kind of what
[00:21:39] you might think of claw doing when it
[00:21:41] comes to navigating the codebase or kind
[00:21:43] of reading docs. Um, it's typically not
[00:21:46] going to run into that problem in step
[00:21:48] one. And it actually could run into that
[00:21:50] problem in step one when you actually
[00:21:52] task a data agent to go solve a problem.
[00:21:55] Uh and this is where in the instructions
[00:21:57] that you actually give it to actually
[00:21:59] write queries, you can actually give it
[00:22:01] the understanding that's an issue and
[00:22:04] that you need to manage your your tokens
[00:22:06] as you're progressing through an
[00:22:07] analysis. Um and the part I want to
[00:22:10] layer in that's really important, you
[00:22:11] need to remind it that the previous
[00:22:13] query it ran had a limit because it
[00:22:15] might actually just take that table it
[00:22:17] queried with a limit 1000 and think it's
[00:22:20] the full table. But being able to kind
[00:22:22] of give it both of those tips to me has
[00:22:24] really unlocked um kind of chain of
[00:22:27] analysis that can lead to a really good
[00:22:29] result that Claude in just like kind of
[00:22:32] like prompting without that was
[00:22:34] struggling to kind of answer. Uh and so
[00:22:37] that was something that I really kind of
[00:22:38] learned firsthand trying to tackle some
[00:22:41] of these analysis questions with quad.
[00:22:44] >> That's a good point. Yeah.
[00:22:46] >> Yeah. Yeah, and I think in that vein in
[00:22:47] what a data MTP is really good at is you
[00:22:49] can actually really limit how much of
[00:22:53] the kind of data set it's actually going
[00:22:55] to use. That means you know we have
[00:22:58] these things called core data inside of
[00:22:59] brack um but the truth is even in that
[00:23:02] core data table for our customers
[00:23:04] there's about eight ways we segment the
[00:23:07] customers and each of them might be
[00:23:09] necessary for a particular use case. Uh,
[00:23:12] I found if you start to kind of give it
[00:23:14] all eight, cloud gets confused, the AI
[00:23:17] agents get confused on which one to use
[00:23:18] and might pick a different one for each
[00:23:20] time it runs through um, an analysis. I
[00:23:25] found like really tight semantic
[00:23:26] context. Just having one of those
[00:23:29] segmentations available also helps
[00:23:32] ensure that it kind of gets the right
[00:23:34] answer. Um, and that, you know, as you
[00:23:36] actually kind of start to layer in more
[00:23:38] and more of that context, not just doing
[00:23:41] the startup data domain that, um, we're
[00:23:44] trying out here, but also the car domain
[00:23:46] and the banking domain or whatever part
[00:23:48] of your product that you're trying to
[00:23:50] add, that tight context is is super
[00:23:52] important. Um, otherwise your kind of
[00:23:54] explorations will get lost or start to
[00:23:56] get wrong. Um, you can't just import
[00:23:58] your whole table into Claude. When you
[00:24:00] say tight semantic context, you mean
[00:24:01] just like telling it which of the 12
[00:24:03] sources to look look at. Is that kind of
[00:24:05] >> Yeah, telling it to only look at one of
[00:24:07] those um semantic context here isn't
[00:24:10] what I'm kind of using to call a
[00:24:12] semantic layer as well. Um so you can
[00:24:15] actually kind of give it that series of
[00:24:17] fields, the dimensions and the
[00:24:19] definitions and some of the use cases of
[00:24:21] using it. But if you add
[00:24:24] >> two that are basically the same, it's
[00:24:27] going to get confused. Um got it. And so
[00:24:29] it's almost like you really just need
[00:24:30] the one and that might limit some of the
[00:24:33] the kind of possibilities, but it
[00:24:35] actually ends up getting you kind of a
[00:24:36] lot further along of something useful in
[00:24:40] um your engagement with the agent.
[00:24:43] >> Okay. Yeah. Because these tables aren't
[00:24:44] and columns aren't necessarily labeled
[00:24:46] well, right? [laughter]
[00:24:48] >> They aren't. Well, I would put it two
[00:24:50] ways. Like sometimes they are and people
[00:24:52] still get confused and there's that. uh
[00:24:55] and sometimes they're not. And you kind
[00:24:57] of have to start to think about some of
[00:24:59] the reasons you want to document your
[00:25:01] tables is for the agents to actually be
[00:25:03] able to pick through what um what fields
[00:25:07] you actually need. And so that kind of
[00:25:09] comes in two ways. Actually, the column
[00:25:11] name is super important, right? It'll
[00:25:13] check that first and just assume doesn't
[00:25:15] always even check on the actual
[00:25:17] documentation. So you want to be clear
[00:25:19] doing explicit naming but then when you
[00:25:21] do have documentation also ensuring that
[00:25:24] that is uh kind of additionally clear. I
[00:25:26] think things that used to not maybe
[00:25:28] matter now matter are also like
[00:25:31] including things like synonyms. Um
[00:25:33] because customer segmentation might be
[00:25:36] you know called motion might be called
[00:25:39] grouping. Those things actually help the
[00:25:41] agent when it comes to processing a
[00:25:43] natural language query actually pick the
[00:25:46] right field. But you don't want to call
[00:25:48] two things grouping, right? Because then
[00:25:50] which one is it going to pick? Um, so
[00:25:52] being kind of clear about that is is
[00:25:54] some of that tight context. And then
[00:25:56] kind of the last one that I found to be
[00:25:57] super useful is kind of what we were
[00:25:59] talking about earlier with you can't
[00:26:01] just rely on only the data set at hand
[00:26:04] to answer a question. If you have uh
[00:26:07] other MCPS connected into your cloud
[00:26:09] code, that can really help craft a
[00:26:11] well-reasoned plan when it comes to uh
[00:26:13] why a particular trend is, you know,
[00:26:16] going in the way it looks. Um, bringing
[00:26:20] in that context is what claude does
[00:26:22] better than sometimes a dedicated BI
[00:26:24] tool where you wouldn't expect to
[00:26:26] connect in your Slack, your entire, you
[00:26:28] know, codebase into those tools to run a
[00:26:32] query. But with cloud cost is pretty low
[00:26:34] to do that and it actually adds a lot of
[00:26:36] value. Again, recreating the way a data
[00:26:39] scientist actually thinks inside of a
[00:26:42] company. They're not just thinking of
[00:26:43] only the data. They're thinking of all
[00:26:45] the things outside of that. um and
[00:26:47] bringing that to the table when they're
[00:26:49] writing analysis like cloud is the first
[00:26:51] kind of cloud code is the first tool I
[00:26:53] feel like could actually accomplish
[00:26:54] that.
[00:26:54] >> Okay. Yeah. I I love this. Um and and
[00:26:58] it's it's all it's all about like
[00:26:59] context management, right? Because like
[00:27:00] you don't want to bring in too much at
[00:27:01] once, otherwise it's going to get
[00:27:02] confused, right? Like like even with
[00:27:04] Slack, you probably don't want to bring
[00:27:05] in like every single channel,
[00:27:07] >> right? Yeah. 100%. You don't want to
[00:27:09] bring in everything. Um you can kind of
[00:27:11] instruct it in that way to kind of look
[00:27:13] through. Um, for us there's like a bunch
[00:27:16] of data help channels or we're
[00:27:18] discussing problems. There's sometimes
[00:27:20] project channels that actually are super
[00:27:22] relevant. So, you can kind of give it
[00:27:24] that prompt as well to look, hey, go
[00:27:26] look at the project channel for this new
[00:27:27] feature, understand how that project's
[00:27:29] going,
[00:27:30] >> uh, and start to write me a query to
[00:27:32] hopefully build a dashboard around when
[00:27:34] we actually ship this project, um, how
[00:27:37] to monitor that. uh and it does it
[00:27:40] surprisingly does that really well which
[00:27:41] is you know exactly what every data
[00:27:44] scientist always wants is to be brought
[00:27:45] along the project and then given this
[00:27:48] one of these tasks go the dashboard you
[00:27:50] kind of had to do that with cloud too
[00:27:52] >> how did you uh how do you connect to
[00:27:54] slack and drive I guess drive is built
[00:27:56] in or
[00:27:58] >> uh yeah with with both of those we
[00:28:00] actually use the glean mcp which we kind
[00:28:03] of added uh and that's kind of like our
[00:28:05] enterprise AI tool that helps connect to
[00:28:07] all these apps So you can actually like
[00:28:09] set that up in cloud code and it can
[00:28:12] access your drive and your Slack to pull
[00:28:15] in the contacts. And so I found it to be
[00:28:17] really helpful to build sub agents for
[00:28:20] each of the connections. Otherwise, it
[00:28:22] might not even know if she needs needs
[00:28:24] to connect to the glean slack part when
[00:28:27] you ask it the question. But if you
[00:28:29] actually build sub agents for each of
[00:28:30] those um it does it pretty well uh when
[00:28:34] you actually like kind of give it the
[00:28:35] instructions.
[00:28:36] >> All right, man. You got you got to make
[00:28:37] a GitHub repo and open source a bunch of
[00:28:39] this the stuff. [laughter]
[00:28:40] >> Yeah.
[00:28:41] >> Yeah.
[00:28:42] >> Uh let me kind of flip back. Awesome. So
[00:28:45] we can see here now we're kind of going
[00:28:47] into MCP that we have actually created
[00:28:50] that startup funding MCP
[00:28:53] um and saving it here in this uh OS
[00:28:56] because I kind of want to
[00:28:58] >> have it uh just for myself. If I was to
[00:29:00] deploy this we'd move it to our kind of
[00:29:02] like centralized repository.
[00:29:04] um you kind of view the tools that we
[00:29:06] created. So there's something here
[00:29:08] around just like I've added that context
[00:29:11] to solve that problem I was mentioning
[00:29:13] around I don't want to blow up the
[00:29:15] context window. I wanted to actually be
[00:29:16] able to like sort through an analysis in
[00:29:20] sequence understanding the limitations
[00:29:22] we put on some of these queries and
[00:29:24] actually report out at the end of every
[00:29:26] analysis like how that's actually
[00:29:28] performing. Um then we just have a basic
[00:29:31] kind of querer that helps translate the
[00:29:34] tables into SQL queries which is you
[00:29:37] know very much the most used part of
[00:29:39] this MCP. Um, and then we kind of have
[00:29:42] the analyze startup trends, which
[00:29:44] basically like an analysis tool to help
[00:29:47] actually bring in some of these like
[00:29:49] common types of analyses and start to
[00:29:51] kind of build the rails of um what the
[00:29:55] sequence of analysis should look like
[00:29:57] when you actually analyze this data set.
[00:29:59] So the query kind of and analyzing tool
[00:30:02] are two of the kind of things that this
[00:30:04] MCP has. And so we can actually try to
[00:30:06] use this um right now. So like um using
[00:30:12] startup data MCP all the most recent
[00:30:17] series A deals in October
[00:30:21] 25 and bring rank them
[00:30:27] by those you think are most likely to
[00:30:31] get a series B. Uh so I've kind of added
[00:30:34] a basic part of this query which is tell
[00:30:37] me the most recent series A deals but
[00:30:39] I'm also adding the analysis piece which
[00:30:41] is I actually wanted to have an opinion
[00:30:43] about what it thinks a series B you know
[00:30:47] potential indicator is and so it kind
[00:30:50] I'm just kind of oneshotting here.
[00:30:52] Normally I might build a plan around
[00:30:53] this but we'll see if it can do it a
[00:30:55] little bit more quickly. What how how do
[00:30:57] you think it's going to look at the data
[00:30:58] to understand series B likelihood like
[00:31:01] it's kind of trajectory or
[00:31:03] >> Yeah, exactly. It'll look at hopefully I
[00:31:05] hope it will look at the past deals that
[00:31:07] got to series B and pick something of a
[00:31:10] criteria. Hopefully it's just like
[00:31:12] funding amounts or speed from series C
[00:31:15] to series A. Um that's where like this
[00:31:18] analysis funding velocity query that was
[00:31:21] created hopefully can give it the rails
[00:31:23] to do that um with a bunch of the kind
[00:31:26] of more interesting. So it it's already
[00:31:29] kind of done this here. There's only two
[00:31:31] series A in October. So first it's
[00:31:33] presenting to me that Glue AI 20 million
[00:31:36] series A and Pedal Surgical 10 million
[00:31:38] series A and AI and health tech. Um and
[00:31:42] it has started to actually rank those.
[00:31:45] Um, and I I can kind of give it some
[00:31:47] questions here on
[00:31:50] why I picked that.
[00:31:51] >> Yeah. Well, hopefully I'll just explain
[00:31:53] why.
[00:31:54] >> Yeah, there you go. Percent rational.
[00:31:55] There you go. At the end. So, yeah,
[00:31:58] here's that report.
[00:31:59] >> Um, so glue AI high probability for
[00:32:04] series A large series A 20 mil, which I
[00:32:07] guess used to be large maybe. So AI
[00:32:10] companies in particular are one of the
[00:32:11] things it thinks will lead it to getting
[00:32:14] a series B. Um I think that it says
[00:32:17] close in October. So you gives an
[00:32:20] expected timeline of a series A domain
[00:32:23] credibility being one somehow thinks
[00:32:25] that that drives you know um a series B
[00:32:30] um and actually predicted what it thinks
[00:32:32] the series B should be.
[00:32:34] >> Yeah. I mean it looks like it's not just
[00:32:35] looking at the data. It's like making
[00:32:37] some rational about like the medical
[00:32:39] stuff taking longer, right?
[00:32:41] >> Yeah. Yeah. Exactly. Yeah. And I'm
[00:32:43] hoping it kind of searched some of that
[00:32:45] within the data set too versus just kind
[00:32:47] of synthes like kind of coming up with
[00:32:50] it itself. Um because right we have that
[00:32:52] data. We have the history of all these
[00:32:54] funding rounds. It should be able to do
[00:32:55] that and it actually does do that right
[00:32:57] based on the series A data. 61% of
[00:32:59] healthcare companies got to series B um
[00:33:03] versus 73% of AI companies. and then
[00:33:06] kind of also looked at kind of the
[00:33:08] median size of a funding round that ends
[00:33:10] up becoming a series B is around 20
[00:33:12] million. So you kind of see that as why
[00:33:15] Glue AI is number one because it has
[00:33:17] that $20 million funding amount. Um
[00:33:20] >> yeah, it's uh it's thinking like a VC
[00:33:22] whenever it sees the word AI in there is
[00:33:24] like oh it's going to be series B.
[00:33:25] [laughter]
[00:33:25] >> Exactly. Yeah. It's already they're
[00:33:27] they're already kind of in the driver's
[00:33:28] seat even if you know we don't even know
[00:33:30] what Google AI does. Um from from the
[00:33:34] perspective of the data set it is
[00:33:36] proving true that those companies do
[00:33:38] receive series B more likely. Uh so for
[00:33:41] that the data says it's correct.
[00:33:43] >> How far back does this data set go? Like
[00:33:45] it's like a year or two or something.
[00:33:47] >> No I think the data set goes back to
[00:33:50] 2016 even beyond. Uh, okay. So, it's
[00:33:54] pretty long historically back because I
[00:33:56] think at some point we had like the
[00:33:57] Amazon funding in there and it was kind
[00:34:00] of like creating some issues. So, we've
[00:34:02] cleaned up some of that, but yeah, we
[00:34:04] have pretty much all historical funding
[00:34:07] rounds. Can I ask a question?
[00:34:09] >> Sure. Yeah.
[00:34:10] >> Can you ask it like something about like
[00:34:12] um you know like whi which AI coding
[00:34:15] tool has the most momentum?
[00:34:20] Yeah,
[00:34:21] >> I guess I Yeah. Do you want to define
[00:34:23] momentum?
[00:34:24] >> Momentum as like, you know, just like,
[00:34:25] you know, which one's going to win, man.
[00:34:27] Like which which one's gonna [laughter]
[00:34:29] Yeah. Which one's going to win?
[00:34:32] >> There you go. Yeah. Why? Yeah, it's good
[00:34:34] to explain why.
[00:34:35] >> Yeah, I want to always give it the
[00:34:37] prompt to use the data because from this
[00:34:39] last example, I worry it's going to just
[00:34:41] try to guess, but So, here's that um
[00:34:43] question. So, it's going to pull all
[00:34:45] these coding tools.
[00:34:46] >> Awesome. Yeah, there there has to be
[00:34:48] like a lot of Yeah.
[00:34:50] >> There's a lot. Yeah, they're starting up
[00:34:52] all the time. Um, yeah, it's it's
[00:34:56] interesting. I'll see what it says. Um,
[00:34:58] here.
[00:34:58] >> Yeah, as you can see, kind of just those
[00:35:00] three queries setting things up. Now, we
[00:35:02] can start to ask these questions. It
[00:35:04] kind of is getting pretty solid results
[00:35:07] using that base set and think of why an
[00:35:10] MTB matters. Now we can actually have
[00:35:11] this created for for you the PMs that
[00:35:15] are trying to access this, the engineers
[00:35:17] that are trying to access this. We're
[00:35:18] kind of building on top of the same uh
[00:35:20] initial set of data.
[00:35:22] >> But like those those like three or four
[00:35:23] queries that you wrote is like really
[00:35:25] important, right? Because like it's
[00:35:26] basically just kind of reusing and those
[00:35:28] those queries each time to get the
[00:35:30] information.
[00:35:31] >> Yeah, it is kind of trying to create
[00:35:33] some new ones too. You kind of see like
[00:35:34] they're not exactly just copying it
[00:35:36] over. But you're right, like those three
[00:35:38] queries, trying to pick kind of ones
[00:35:40] that have joins in them that have
[00:35:42] multiple tables, uh, that have some like
[00:35:45] aspect of an analysis you might want to
[00:35:48] build upon is is pretty important. And
[00:35:50] and that's exactly also where I don't
[00:35:52] want to just invent them. I actually
[00:35:54] want to look to see what queries people
[00:35:56] are running against this table and start
[00:35:58] to pick from that. Um, got that way you
[00:36:00] don't have to like kind of create
[00:36:01] something that isn't actually following
[00:36:03] the current way folks are quering
[00:36:06] things.
[00:36:06] >> And um,
[00:36:08] >> the actual MCP uh that you built for
[00:36:10] Brexit like uh can can anyone like can
[00:36:13] PMs use it and stuff?
[00:36:15] >> Yeah, absolutely. Yeah, the way we built
[00:36:17] the data MCP is you all you need to do
[00:36:19] is like authorize or snowflake connect
[00:36:22] into the MTP and it should be able to
[00:36:24] run whenever you're asking a data
[00:36:26] question. and it'll write the query and
[00:36:28] actually generate you the result and
[00:36:30] answer and then you can build your own
[00:36:32] agents to kind of build your dashboard
[00:36:34] version uh okay set of those queries on
[00:36:37] a recurring basis. I can kind of show
[00:36:38] you um at least what I built on this
[00:36:41] particular MCP a version of that
[00:36:43] dashboard. But here's the uh answer.
[00:36:45] >> Yeah, let's take a look.
[00:36:46] >> You're looking for um
[00:36:48] >> let's take a look.
[00:36:49] >> Yeah, so based on the data driven
[00:36:51] momentum indicators cursor, no surprise
[00:36:54] here. Uh huge series A. The valuation is
[00:36:59] a little lagging. See, I think 400
[00:37:01] million plus isn't actually correct. So
[00:37:03] we can try to maybe investigate that.
[00:37:06] Believe now they're 10. They're 30
[00:37:08] billion. But um kind of the key signal
[00:37:10] A16Z backing um is solid. Uh it also has
[00:37:15] some interesting research on why uh you
[00:37:18] think it's kind of uh a top tool. Um
[00:37:22] Replet
[00:37:24] another one kind of number two ranked
[00:37:27] ahead of I guess kodium windsurf
[00:37:31] cognition right? Um yeah, one of the
[00:37:33] issues in this data set is startups
[00:37:35] change their name and they get acquired
[00:37:37] and so you kind of lose some of the
[00:37:39] thread sometimes uh on those shifts, but
[00:37:42] um these are kind of the top three it
[00:37:44] thinks are going to win.
[00:37:45] >> Yeah, I believe one or two I'm not so
[00:37:47] sure about three, but you know one one
[00:37:49] or two are in good shape. Yeah.
[00:37:51] >> Yeah. I I think the data we've seen
[00:37:54] definitely agrees with some of that. Um
[00:37:56] >> Okay, nice. This is awesome. This is
[00:37:59] fun. Yeah. Well, let's let's go back to
[00:38:01] my original question because like uh at
[00:38:03] companies with with data teams, there's
[00:38:05] always a concern that if you get some
[00:38:07] random PM like access to like these
[00:38:09] queries like you know they'll just like
[00:38:11] run a random query that just like take
[00:38:13] takes down the whole database or like
[00:38:14] [laughter]
[00:38:15] right so so like um and with this MCB
[00:38:18] stuff it can actually incur a bunch of
[00:38:19] token costs right if someone goes off
[00:38:21] the rails. So so I'm just curious do you
[00:38:22] have any controls in place? Yeah, I
[00:38:25] think that's that's something where I
[00:38:27] find like the new thing that cloud
[00:38:30] release skills actually kind of solves
[00:38:31] where you can actually design kind of
[00:38:33] these these skills the agents tap into
[00:38:36] when it comes to building analyses or
[00:38:39] reporting to instruct it not to do that.
[00:38:42] And unlike kind of a PM like the skill
[00:38:44] will follow it, right? It'll be
[00:38:46] cognizant of the tokens it's using to
[00:38:50] make sure it doesn't take down the
[00:38:51] database. um it'll kind of point out the
[00:38:55] areas where it potentially is making
[00:38:57] kind of leaps in terms of how it's
[00:38:59] joining things together
[00:39:01] >> uh and kind of reveal those. So as you
[00:39:03] you know as a PM maybe start to try to
[00:39:05] productionize something or share it with
[00:39:07] your data person at least there's some
[00:39:09] way for that data person to like quickly
[00:39:11] understand what is the area I need to
[00:39:14] think about um helping with or fixing.
[00:39:18] Uh so we found that like actually to be
[00:39:20] something that is pretty useful when it
[00:39:22] comes to like creating these skills and
[00:39:23] we have like I have created a few skills
[00:39:25] here to um actually help with that like
[00:39:29] the ad hoc uh analysis skill. Here's
[00:39:33] like the um data analysis skill I have.
[00:39:36] Um
[00:39:37] >> Oh, wow.
[00:39:38] >> that like runs a bunch of of these kind
[00:39:40] of context management
[00:39:42] um ideas, the computational story
[00:39:45] mapping. Um to think about these SQL
[00:39:47] queries as like a narrative versus just
[00:39:49] individual components. Being able to
[00:39:52] share this skill with a PM and tell them
[00:39:54] to use that at least gives me some
[00:39:55] confidence. It won't completely take
[00:39:57] down the database.
[00:39:59] >> Um and that's something that's new,
[00:40:00] right? like you can't necessarily always
[00:40:02] teach someone that logic um and have
[00:40:05] them follow it every time, but skills
[00:40:08] you could.
[00:40:09] >> Yeah, I haven't I haven't dug into
[00:40:10] skills, but it sounds so so it's not
[00:40:12] going to let me join like two million
[00:40:14] two million row tables together. It's
[00:40:16] not going to ask. [laughter]
[00:40:17] It's not
[00:40:17] >> Yeah, that's that's like the number one
[00:40:19] issue where you could just fill your
[00:40:20] contact window up like with a single
[00:40:22] query. But with the skills, you can tell
[00:40:24] it, hey, like put a limit 50 on any of
[00:40:27] those types of joins. So it doesn't
[00:40:29] completely take down your database and
[00:40:31] then have a timeout alongside that to
[00:40:33] say like queries are taking longer than
[00:40:35] two to three minutes. Try to rewrite
[00:40:37] them, kill it, rewrite it to make it a
[00:40:40] little bit more performant.
[00:40:42] >> Those are the pieces of what I'm really
[00:40:44] excited about. the self-service aspect
[00:40:46] of not just sharing like this query and
[00:40:49] having someone modify it, actually
[00:40:51] teaching quad code, how to do a data
[00:40:55] analysis, have that to be something that
[00:40:57] folks can tap into along with data
[00:40:59] visualizations,
[00:41:01] common types of analysis like cohort
[00:41:03] analysis
[00:41:04] >> um
[00:41:05] >> and so on. So for that it's um even down
[00:41:08] to like sometimes I was trying to like
[00:41:10] export this CSV to go do something with
[00:41:13] it and it was kind of creating a bit of
[00:41:15] annoyance created a skill to export CSVs
[00:41:18] [snorts] and now I can kind of tap into
[00:41:20] that as you try to uh kind of move data
[00:41:23] around your system.
[00:41:24] >> All right we we got to do a separate
[00:41:25] episode on skills at some point.
[00:41:27] >> Okay. [laughter]
[00:41:28] Yeah, skills are super important. But
[00:41:30] let's let's take the last few minutes.
[00:41:32] Let's talk about some of the data that
[00:41:33] you already p and that you shared public
[00:41:34] blog post about. Like there's some
[00:41:36] pretty interesting data based on Brexit
[00:41:38] spending patterns, right?
[00:41:40] >> Yeah, absolutely. Yeah, we've been
[00:41:41] publishing these kind of data blogs for
[00:41:45] since I think for almost like six plus
[00:41:48] months and it's been really interesting
[00:41:50] to see from our customers what's
[00:41:52] winning. Right? You asked what's the
[00:41:54] best IDE or kind of coding tool. Well,
[00:41:58] the customers on our kind of platform
[00:42:00] have already picked it is definitely
[00:42:02] cursor. Cursor is absolutely kind of
[00:42:04] crushing it in like what um is the
[00:42:09] startup coding tool of choice but also
[00:42:11] enterprise coding tool of choice. And
[00:42:13] that mix of being able to like tap into
[00:42:16] both of those um sets of customers
[00:42:18] within Brax is pretty rare to see them
[00:42:21] kind of like succeed across both uh both
[00:42:24] startups and enterprise and kind of
[00:42:26] proving why they're on pace for their
[00:42:28] $30 billion funding round. So for for me
[00:42:31] that that stuff starting to show up
[00:42:33] pretty clearly and they've been number
[00:42:34] three for a few months now. It's also
[00:42:37] not like a temporary thing.
[00:42:38] >> That that's awesome. Yeah. And and
[00:42:39] there's some other really interesting
[00:42:41] stuff on here like well I guess the
[00:42:42] these aren't like um in a linear scale
[00:42:45] right probably like the you know it's
[00:42:46] probably yeah but but but still it's
[00:42:48] interesting to see something like 11
[00:42:49] labs on here relatively close to top I
[00:42:52] guess people are building like voice
[00:42:53] assistants or something
[00:42:54] >> or
[00:42:55] >> yeah they're kind of the voice assistant
[00:42:57] of choice like whenever there's any of
[00:42:59] this like kind of voice added to your
[00:43:01] platform especially within startups like
[00:43:03] I wouldn't think of it as much as like
[00:43:05] that startup is using it for their own
[00:43:07] internal use cases they're adding those
[00:43:09] tools to their their apps, to their
[00:43:11] platform, to their products. That's
[00:43:13] where 11 Labs really kind of like spiked
[00:43:15] in terms of they're they're almost
[00:43:18] always the first choice for those voice
[00:43:22] um kind of assistant add-ons. Um so it's
[00:43:25] it's pretty clear to me that like
[00:43:26] they've been winning in that space
[00:43:28] >> like customer support and stuff like
[00:43:29] that or or Yeah.
[00:43:31] >> Yeah. I mean we have to look at the
[00:43:33] element labs kind of like use cases but
[00:43:35] yeah definitely um succeeding in being
[00:43:39] that first choice and it's something we
[00:43:40] kind of saw as well in like the um
[00:43:43] bundle analysis here
[00:43:45] >> where I can probably see the yeah for
[00:43:48] voice apps um you know there's obviously
[00:43:51] other kind of voice apps that are
[00:43:54] >> popping deep speechify assembly AI
[00:43:57] >> but definitely um the the 11 labs has
[00:44:03] been kind of the most consistent uh in
[00:44:06] terms of choice and and where the
[00:44:07] dollars are end ending up going. I also
[00:44:10] think like it's very interesting the the
[00:44:12] top two like um
[00:44:14] >> the the fact that the fact that open AAI
[00:44:16] is more popular for enterprise than
[00:44:17] anthropic because anthropic really
[00:44:19] targeting companies right so
[00:44:22] >> I was seeing that in the data where in
[00:44:24] 2024 it was opening across both startups
[00:44:26] and enterprise they they were winning
[00:44:28] both but starting in 2025 we started
[00:44:31] seeing a lot of startups shift some of
[00:44:33] the ship some of their agentic kind of
[00:44:36] AI products in their apps
[00:44:39] And that's where in startups if you're
[00:44:41] doing that a lot of them were choosing
[00:44:44] claude as the model to put into their
[00:44:47] production apps.
[00:44:48] >> Um you know that kind of like call it
[00:44:50] enterprisegrade type model which is you
[00:44:53] know a little bit kind of more reliable
[00:44:55] or whatever it is the reasons on cloud
[00:44:58] has been the choice for a lot of
[00:44:59] startups. So we see the agentic features
[00:45:02] within startups products are often times
[00:45:05] picking claude over open AI and that's
[00:45:08] where the startup spend on anthropic is
[00:45:10] going up versus within enterprise
[00:45:12] >> it's a bit more of the like chatbt pro
[00:45:15] licenses or the enterprise licenses that
[00:45:17] are helping it boost its rankings here.
[00:45:19] Um, so we see a lot of spend going to
[00:45:22] anthropic within startups. Uh, and folks
[00:45:24] are starting to add Claude as well as
[00:45:26] like a subscription, but it's usually
[00:45:28] kind of the second subscription.
[00:45:30] >> Yeah,
[00:45:31] >> Open Eyes is still winning on the chat
[00:45:33] side.
[00:45:33] >> Yeah, I I guess you know enterprises way
[00:45:35] more than than just all the tech
[00:45:36] companies, right? There's like, you
[00:45:37] know, Yeah,
[00:45:38] >> there's like uh like all the accounting
[00:45:40] companies and all the international
[00:45:41] companies which OpenAI have much better
[00:45:43] brand a awareness, I'm sure.
[00:45:45] >> Yeah, absolutely. Like if you're if
[00:45:46] you're one of those companies, you're
[00:45:48] just signing up for OpenAI because what
[00:45:50] we see in our data is there was actually
[00:45:53] five employees already purchasing the
[00:45:54] pro subscription, you didn't have the
[00:45:56] team set up. There was, you know, maybe
[00:45:58] a little bit of security risk there. So
[00:46:00] you just onboard that entire tool and
[00:46:02] give it to all your employees because
[00:46:04] you got to mandate you got to be AI, you
[00:46:07] know, an AI oriented company. It's like,
[00:46:09] oh, let's just get in open AI. Um yeah
[00:46:11] versus startups that are building these
[00:46:13] new solutions they have a lot of
[00:46:14] opinions about which model why they want
[00:46:16] to use that model and uh we're seeing
[00:46:18] yeah anthropic win out a lot of those
[00:46:21] those choices
[00:46:22] >> but dude I think overall it's like a
[00:46:23] blood bath man like I I think the top
[00:46:26] top five are okay or I mean the top
[00:46:28] seven but like you know there's like
[00:46:30] thousands of AI companies man trying to
[00:46:31] go after these com these these
[00:46:32] enterprises and startups. Yeah, seeing
[00:46:35] really interesting data like coming
[00:46:37] through where startups that were
[00:46:39] launched in 2024 June are really
[00:46:42] crushing it like absolutely like you
[00:46:46] know straight line logarithmic growth in
[00:46:48] terms of the adoption because they were
[00:46:49] able to build a component of the AI
[00:46:53] workflow. their agents do really well
[00:46:55] and so like startups are adding in that
[00:46:57] specific agent for that workflow um and
[00:47:01] as a result spending a lot on that and
[00:47:03] it's kind of interesting we'll kind of
[00:47:05] release something um probably in
[00:47:07] December overviewing kind of the
[00:47:09] industry overall in terms of AI and how
[00:47:11] those those sometimes those specific use
[00:47:14] cases are helping companies uh gain
[00:47:17] traction in the space. Well, you should
[00:47:19] read his blog post on like because I'm
[00:47:21] I'm sure you see like a lot of
[00:47:22] [laughter] AI companies get a ton of
[00:47:24] traction and then they they go down.
[00:47:26] Maybe when open I c h c h c h c h c h c
[00:47:27] h c h c h c h c hobbies is fe feature or
[00:47:28] something. Yeah.
[00:47:29] >> Yeah, it's true. It goes up and down,
[00:47:31] right? It's like um and that's where
[00:47:33] checking in on it every month which you
[00:47:34] do in the Rex benchmark. It's been
[00:47:36] really interesting.
[00:47:37] >> Uh you know I think Replet's a really
[00:47:39] great story even though they kind of
[00:47:40] dropped this month. They weren't really
[00:47:42] even in this list the first time we
[00:47:44] created it, but they're kind of shipping
[00:47:46] new and newer and newer agentic kind of
[00:47:49] capabilities and yeah have kind of
[00:47:51] started to kind of enter into the top 10
[00:47:54] for startups and enterprise.
[00:47:57] So really impressive that team and and
[00:47:59] what they're shipping.
[00:48:00] >> Yeah, that is awesome man. And and and
[00:48:02] and you know the more agent ticket it is
[00:48:04] the more tokens it cost. So you know
[00:48:06] just
[00:48:07] >> yeah more
[00:48:08] >> and that ends [clears throat] up being
[00:48:09] spent on bre. So we're happy um yeah for
[00:48:12] that. Yeah, sure.
[00:48:14] >> Okay. All right, man. Okay, that that
[00:48:16] that was a good segue, but uh but let
[00:48:18] let me wrap up with this. You know,
[00:48:19] people want to scale up on on this
[00:48:20] stuff, right? And it just showed how you
[00:48:22] can build an MCP to start pulling
[00:48:24] queries and asking questions yourself
[00:48:26] like uh you know, if if like a data
[00:48:28] scientist or like a PM wants to get
[00:48:30] started with this stuff like what are
[00:48:32] your top three steps, man? Like how how
[00:48:34] can you get started becoming like
[00:48:35] Summit? Yeah. [laughter]
[00:48:37] >> Yeah. I think I think it is that idea
[00:48:39] that like even as you think about these
[00:48:42] BI tools that are all selling you this
[00:48:45] AI workflow, you still have to do the
[00:48:48] work to kind of pick those core queries,
[00:48:51] write the contacts, and even if you just
[00:48:53] end up plugging it into something like
[00:48:55] hack, which has been absolutely crushing
[00:48:56] it on their agentic analysis tools, you
[00:48:59] still need to have an opinion about like
[00:49:01] what are the queries going into that?
[00:49:03] what is the context I want to give um
[00:49:06] the idea about how to think about our
[00:49:07] data and how to write analysis and the
[00:49:09] common types of analysis. So it's super
[00:49:11] important for everyone to really start
[00:49:13] to add that to the scope of what a data
[00:49:16] team works on. Um you you going to be
[00:49:19] able to one leverage it inside of cloud
[00:49:21] code for all of engineering which is
[00:49:23] great. Um but you're also then going to
[00:49:26] be able to plug it into these BI tools.
[00:49:29] So for me, getting those three queries,
[00:49:31] starting to write context about your
[00:49:33] specific domain that you care about is
[00:49:35] the perfect way to build out your data
[00:49:39] MCP, but also maybe you just throw that
[00:49:41] right into the BI tools and it starts
[00:49:44] working better too. Those AI tools
[00:49:46] within hacks like need that to actually
[00:49:49] get to the right answer as well.
[00:49:51] >> Got it. So those three queries are more
[00:49:54] like are they more like questions you
[00:49:56] have on database or or just capabilities
[00:49:58] that you want the AI to to have is
[00:50:02] there's more capabilities right like
[00:50:03] that that you want
[00:50:04] >> yeah more capabilities like there's a
[00:50:06] bunch of value to it it's like even just
[00:50:07] understanding how you want to structure
[00:50:09] the query what those common common joins
[00:50:11] are but also the components of analysis
[00:50:13] where you know you want to know what the
[00:50:15] top startup funding rounds are you want
[00:50:17] to know what the top VCs are that are
[00:50:19] funding those startups how do you kind
[00:50:21] of bridge those two together. You don't
[00:50:23] even need to do that. You just need to
[00:50:24] give it the components and it can kind
[00:50:25] of fill in the gaps on how to like join
[00:50:27] this series of tables to this series
[00:50:29] tables to find the top VCs finding the
[00:50:31] top startups. It's exactly kind of what
[00:50:33] we saw in those examples. Um so having
[00:50:37] kind of basic use cases plus one larger
[00:50:40] analysis query
[00:50:43] >> is enough to kind of start the agentic
[00:50:46] AI journey.
[00:50:47] >> Okay. So the TRDR is like uh the context
[00:50:50] management is like super important like
[00:50:52] uh both giving AI the right context not
[00:50:54] overloading it window and also giving it
[00:50:57] like the right capabilities to to look
[00:50:58] into your stuff right
[00:51:01] >> yeah that's well said
[00:51:03] >> cool man all right dude so where can
[00:51:04] people find you uh on LinkedIn or you
[00:51:07] know where can people find your stuff
[00:51:09] >> you can follow me on Twitter at simma ma
[00:51:12] ma and then you know also check out the
[00:51:14] brakes blog we're publishing benchmarks
[00:51:17] every single month. So, working on a
[00:51:19] bunch of really cool things that I hope
[00:51:21] that you could get a check out.
[00:51:23] >> Yeah, it's really good data, man. It's
[00:51:24] like way better than looking at all the
[00:51:25] AR numbers. It's good. It's actually
[00:51:28] data.
[00:51:28] >> AR numbers, funding round data. It tells
[00:51:31] you a story, but we're seeing actual
[00:51:33] spend and that can kind of reveal what
[00:51:36] actually is being used in market versus
[00:51:38] the hype around some big number. Yeah.
[00:51:41] >> Cool. All right. So, well, thanks so
[00:51:43] much, man. I I learned so much.
[00:51:44] >> Thanks, Peter. Yeah, I had a had a
[00:51:46] blast.
