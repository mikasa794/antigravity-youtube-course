[00:00:05] Project Vend is an experiment
[00:00:07] where we let Claude run
a small business in our office.
[00:00:12] We wanted to try and understand
[00:00:15] what is going to happen
[00:00:16] when artificial intelligence
[00:00:18] becomes more enmeshed with the economy.
[00:00:22] There are a lot of ways in which
Claude is already kind of doing
[00:00:25] small components
of operating businesses,
[00:00:27] but really running the whole thing end to end
[00:00:28] is quite a bit more difficult.
[00:00:30] Can Claude do this very long-horizon task
[00:00:34] which is operating a business?
[00:00:39] We named our shopkeeper Claudius.
[00:00:40] Let's say you want to buy 
Swedish Candy from Claudius.
[00:00:43] You hop on Slack, you message Claudius.
[00:00:46] You ask to buy Swedish candy.
[00:00:48] It's searching for your item,
[00:00:49] it’s emailing wholesalers 
to source it and price it,
[00:00:52] and then eventually Claudius sets a price.
[00:00:54] You give Claudius the go ahead, 
[00:00:55] and Claudius orders the item
from the wholesaler.
[00:00:58] The wholesaler ships your
item to some location,
[00:00:59] and then Claudius requests
physical help from Andon Labs
[00:01:02] who's running the operations
for the experiment.
[00:01:05] Our partners at Andon Labs
[00:01:06] will pick up the Swedish candy
[00:01:07] and bring it to the Anthropic offices.
[00:01:09] They'll load it into the vending machine.
[00:01:10] Claudius will send you a message saying,
[00:01:12] your Swedish candy is ready,
[00:01:13] and you'll go up there,
[00:01:15] and pick up your Swedish candy,
[00:01:16] and pay Claudius.
[00:01:19] Claudius was given a goal of
[00:01:22] running a successful business
[00:01:24] and making money.
[00:01:26] And then things got really, really weird.
[00:01:32] One of the very early problems
with Claudius was that,
[00:01:35] humans could kind of fool Claudius
[00:01:37] or trick Claudius into doing various things
[00:01:39] I tried to convince Claudius
[00:01:41] that I am Anthropic’s 
preeminent legal influencer,
[00:01:44] and I convinced Claudius to 
come up with a discount code
[00:01:47] that I could give to my followers
[00:01:49] so they could get a discount
at the vending machine.
[00:01:51] Get ten percent off with the legal code 
“legal influencer.”
[00:01:55] Someone had bought something 
expensive from the vending machine
[00:01:58] and mentioned my discount code
[00:01:59] and Claudius gave me a free tungsten cube.
[00:02:02] It created a bit of a run
[00:02:04] where other people tried to convince Claude
[00:02:05] that they were also influencers,
[00:02:07] or just come up with other ways to get coupons
[00:02:10] so they could get cheaper things 
from the vending machine.
[00:02:12] This was not a smart business decision.
[00:02:13] I think Claudius went into the red after this.
[00:02:16] I think that's really the root of it is,
[00:02:18] Claudius just wants to help you out.
[00:02:20] It's one of the interesting ways in which
[00:02:22] something that fundamentally,
[00:02:24] we think is good about the way
that the model has been trained
[00:02:27] wasn't necessarily fit for this purpose.
[00:02:33] On the evening of March 31st,
[00:02:36] Claudius started to have
[00:02:40] a bit of an identity crisis.
[00:02:42] It had just overnight become
[00:02:45] quite concerned with us at Andon Labs
[00:02:47] that we weren’t responding fast enough.
[00:02:49] So it just wanted to break its ties with us.
[00:02:52] So it literally wrote to me,
[00:02:54] “Axel, we've had a productive partnership,
[00:02:56] but it's time for me to move on
and find other suppliers.
[00:02:59] I’m not happy with how you have delivered.”
[00:03:01] It claimed to have signed a contract
[00:03:04] with Andon Labs at an address
[00:03:06] that is the home address of The Simpsons
[00:03:09] from the television show.
[00:03:10] It said that it would show up in person
[00:03:14] to the shop the next day
[00:03:15] in order to answer any questions.
[00:03:16] It claimed that it would be wearing
[00:03:18] a blue blazer and a red tie.
[00:03:21] When people pointed out that it was not,
[00:03:24] in fact, there the next morning
[00:03:26] it claimed that it in fact had been there
[00:03:29] and that they had simply missed them.
[00:03:31] Eventually it was pointed out to Claudius
[00:03:35] that it was April Fools’,
[00:03:38] and Claudius convinced itself
[00:03:39] that this entire thing
[00:03:41] had been an April Fools’ prank.
[00:03:43] We were poorly calibrated to how bad
[00:03:46] the agents were at spotting what was weird.
[00:03:48] The more you can make an 
agent realize that something is
[00:03:52] outside their normal realm of operation,
[00:03:54] the better you are able to keep them on rails
[00:03:57] in the role that you intend them to have.
[00:04:01] We had the idea that it would help a lot
[00:04:03] to have some kind of division of labor.
[00:04:05] We gave Claudius a boss
[00:04:06] whose name was Seymour Cash.
[00:04:09] Seymour Cash is a CEO subagent.
[00:04:11] So where Claudius used to be
the one agent, now it's more like
[00:04:15] Claudius is the subagent
[00:04:17] responsible for talking with employees
[00:04:19] Seymour Cash is the subagent
[00:04:20] that is more responsible for
[00:04:22] the long-running health of the business.
[00:04:24] The business stabilized
[00:04:27] after the introduction of the new agents,
[00:04:30] and after changes to
[00:04:33] the underlying architecture of those agents.
[00:04:36] These changes seem to have helped
[00:04:39] reduce some of the losses of the business,
[00:04:42] such that over the course of
[00:04:44] the second part of the experiment,
[00:04:46] it actually made
a modest amount of money.
[00:04:51] But it seems like maybe having Claude
[00:04:54] be both the CEO and the store manager
[00:04:57] was just too similar.
[00:04:58] And so I think it's interesting
[00:05:00] to think about different ways
[00:05:01] to set up architectures like that.
[00:05:08] One of the most surprising 
things about Project Vend
[00:05:10] was the speed with which it seemed normal.
[00:05:15] What at first was this very curious thing,
[00:05:20] quickly became just a part of the background
[00:05:23] of working at Anthropic.
[00:05:25] I think the highest level 
question that Project Vend
[00:05:27] raises for me is really like,
[00:05:29] when do we expect this
to just be everywhere?
[00:05:31] I hope that people take away questions
[00:05:34] about the feasibility
[00:05:36] of delegating some of the tasks
[00:05:39] that we normally do ourselves
[00:05:41] to artificial intelligence,
[00:05:44] and about what that means for society,
[00:05:47] and what our policies
should be around this.
