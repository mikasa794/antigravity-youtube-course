[00:00:04] How sure are you that you can tell
[00:00:06] what's real online?
[00:00:09] You might think it's easy to spot an
[00:00:10] obviously AI generated image and you're
[00:00:12] probably aware that algorithms are
[00:00:14] biased in some way, but all the evidence
[00:00:16] is suggesting that we're pretty bad at
[00:00:18] understanding that on a subconscious
[00:00:20] level. Take for example the growing
[00:00:22] perception gap in America. We keep over
[00:00:24] and overestimating how extreme other
[00:00:26] people's political beliefs are. This is
[00:00:28] only getting worse with social media
[00:00:29] because algorithms show us the most
[00:00:31] extreme picture of reality. As an
[00:00:34] etmologist and content creator, I always
[00:00:36] see controversial messages go more viral
[00:00:38] because they generate more engagement
[00:00:39] than a neutral perspective. But that
[00:00:42] means we all end up seeing this more
[00:00:43] extreme version of reality and we're
[00:00:45] clearly starting to confuse that with
[00:00:47] actual reality. The same thing is
[00:00:50] currently happening with AI chat bots
[00:00:52] because you probably assume that chatbt
[00:00:53] is speaking English to you except it's
[00:00:55] not speaking English. In the same way
[00:00:57] that the algorithm is not showing you
[00:00:59] reality, there are always distortions
[00:01:01] depending on what goes into the model
[00:01:03] and how it's trained. Like we know that
[00:01:05] Chad says delve at way higher rates than
[00:01:07] usual, possibly because OpenAI
[00:01:09] outsourced its training process to
[00:01:10] workers in Nigeria who do actually say
[00:01:12] delve more frequently. Over time though,
[00:01:15] that little linguistic over
[00:01:16] representation got reinforced into the
[00:01:18] model even more than in the workers own
[00:01:19] dialects. Now that's affecting
[00:01:21] everybody's language. Multiple studies
[00:01:23] have found that since Chad GPTt came
[00:01:24] out, people everywhere have been saying
[00:01:26] the word delmore in spontaneous spoken
[00:01:28] conversation.
[00:01:30] Essentially, we're subconsciously
[00:01:32] confusing the AI version of language
[00:01:34] with actual language. But that means
[00:01:36] that the real thing is ironically
[00:01:37] getting closer to the machine version of
[00:01:39] the thing. We're in a positive feedback
[00:01:41] loop with the AI representing reality,
[00:01:43] us thinking that's the real reality, and
[00:01:45] then regurgitating it so that the AI can
[00:01:46] be fed more of our data. You can also
[00:01:49] see this happening with the algorithm
[00:01:50] through words like hyperpop which wasn't
[00:01:52] really part of our cultural lexicon
[00:01:53] until Spotify noticed an emerging
[00:01:55] cluster of similar users in their
[00:01:56] algorithm. As soon as they identified it
[00:01:59] and introduced a hyperpop playlist,
[00:02:00] however, the aesthetic was given a
[00:02:02] direction. Now, people began to debate
[00:02:04] what did and did not qualify as
[00:02:06] hyperpop. The label and the playlist
[00:02:08] made the phenomenon more real by giving
[00:02:10] them something to identify with or
[00:02:12] against. And as more people identified
[00:02:13] with hyperpop, more musicians also
[00:02:15] started making hyperpop music.
[00:02:18] All the while, the cluster of similar
[00:02:20] listeners and the algorithm grew larger
[00:02:22] and larger and Spotify kept pushing it
[00:02:23] more and more because these platforms
[00:02:25] want to amplify cultural trends to keep
[00:02:27] you on the app. But that means we also
[00:02:30] lose the distinction between a real
[00:02:31] trend and an artificially inflated
[00:02:33] trend. And yet this is how all fads now
[00:02:36] enter the mainstream.
[00:02:38] We start with a latent cultural desire
[00:02:39] like maybe some people are interested in
[00:02:41] matcha or labu or Dubai chocolate. The
[00:02:43] algorithm identifies this desire and
[00:02:45] pushes it to similar users, making the
[00:02:47] phenomenon more of a thing. But again,
[00:02:49] just like how chatp misrepresented the
[00:02:50] word delve, um the algorithm is probably
[00:02:53] misrepresenting reality. Now, more
[00:02:54] businesses are making labu content
[00:02:56] because they are think that's the
[00:02:58] desire. More influencers are also making
[00:03:01] labu trends because we have to tap into
[00:03:02] trends to go viral. And yet the
[00:03:05] algorithm is only showing you the
[00:03:07] visually provocative items that work in
[00:03:08] the video format.
[00:03:11] Tik Tok has a limited idea of who you
[00:03:14] are as a user and there's no way that
[00:03:15] matches up with your complex desires as
[00:03:17] a human being. So, we have a biased
[00:03:18] input and that's assuming that social
[00:03:20] media is trying to faithfully represent
[00:03:22] reality, which it isn't. Instead, it's
[00:03:25] only trying to do what's going to make
[00:03:26] money for them. It's in Spotify's
[00:03:28] interest to have you listening to
[00:03:28] hyperpop and it's in Tik Tok's interest
[00:03:30] to have you looking at the boo boos
[00:03:31] because that's commodifiable. So once
[00:03:33] again, we have this difference between
[00:03:35] reality and the representation of
[00:03:37] reality where they're actually
[00:03:38] constantly influencing one another.
[00:03:42] But it's incredibly dangerous to ignore
[00:03:44] that distinction because this goes
[00:03:45] beyond our language and our consumptive
[00:03:47] behaviors. This affects the world we see
[00:03:49] as possible. Evidence suggests that
[00:03:51] chatbt is more conservative when
[00:03:53] speaking the Farsy language, likely
[00:03:54] because the limited training texts in
[00:03:56] Iran reflect the more conservative
[00:03:58] political climate in the region. Does
[00:03:59] that mean that an Iranian chat GPT user
[00:04:01] will think more conservative thoughts?
[00:04:03] We know that Elon Musk regularly makes
[00:04:06] changes to his chatbot Grock when he
[00:04:07] doesn't like how it's responding and
[00:04:09] that he uses his platform X to
[00:04:10] artificially amplify his tweets. Does
[00:04:13] that mean that the millions of Grock and
[00:04:14] ex users are subconsciously being
[00:04:16] trained to align with Musk's ideology?
[00:04:19] We need to constantly remember that
[00:04:21] these aren't neutral tools. Everything
[00:04:24] that ends up in your social media feed
[00:04:26] or in your chatbot responses is actually
[00:04:27] filtered through many layers of what's
[00:04:29] good for the platform, what makes money,
[00:04:31] and what conforms to the platform's
[00:04:33] incorrect idea about who you are. When
[00:04:36] we ignore this, we view reality through
[00:04:38] a constant survivorship bias, which
[00:04:40] affects our understanding of the world.
[00:04:42] After all, if you're talking more like
[00:04:44] ChattyBT, you're probably thinking more
[00:04:47] like Chat GBT as well, or Tik Tok, or
[00:04:50] Spotify. But you can fight this if you
[00:04:52] constantly ask yourself why. Why am I
[00:04:55] seeing this? Why am I saying this? Why
[00:04:58] am I thinking this? And why is the
[00:05:00] platform rewarding this? If you don't
[00:05:02] ask yourself these questions, their
[00:05:04] version of reality is going to become
[00:05:05] your version of reality. So stay real.
[00:05:10] [applause]
